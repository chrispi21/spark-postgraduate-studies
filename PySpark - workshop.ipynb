{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481836e5-41df-4436-8863-2790eeada5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#  Wprowadzenie\n",
    "\n",
    "W notebook'ach zdefiniowana jest zmienna `spark` typu `SparkSession`, która będzie punktem startowym naszej pracy ze Sparkiem. Tego typu konwencja nazewnicza jest bardzo częsta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1af3aa-ee2f-48dc-a0b2-487c14d441b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27deb15c-d62b-43d4-b1ad-d462b6d7787e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Jak tworzymy Dataframe?\n",
    "Pierwszym sposobem jest użycie Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9ed29c-098a-4528-9723-5271d063003d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers = spark.sql(\"select * from uam_offers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54093dca-d39a-43e7-9e7e-a545971477e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W powyższym przypadku prościej będzie jednak skorzystać z metody `table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8f0423-3572-4215-a1c1-121ed5a57bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers = spark.table(\"uam_offers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e99776-5472-48ef-a237-68352e06c190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metoda `sql` przyjmuje dowolne zapytanie Spark SQL. Np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54a4965-3a99-444c-8792-1e7fdf028820",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show create table uam_offers\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fecc0d-aa51-4056-8c74-25a989e0527f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Możemy również odczytać dane bezpośrednio z plików:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66483e43-dd4f-4639-bab5-619464596f07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location = \"dbfs:/user/hive/warehouse/uam_offers\"\n",
    "uam_offers_from_file = spark.read.format(\"delta\").option(\"path\", location).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac8dba5-9076-4af9-8d94-3c9baec56b9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Drugi sposób wymaga od nas znajomości formatu pliku (w typ przypadku `delta`) oraz ścieżki.\n",
    "\n",
    "\n",
    "Kolejnym sposobem jest wykorzystanie metody `createDataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95b4a8d-bb8f-426c-bc6a-67717e2a712d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wersja 1: \n",
    "# Pierwszy argument: lista tupli\n",
    "# Drugi (opcjonalny) argument: lista string (nazwy kolumn)\n",
    "spark.createDataFrame([(1, 2), (11, 22)], ['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1061cbda-9d68-4388-a36c-eddce6583fce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wersja 2:\n",
    "# Wykorzystujemy klasę Row, która pozwala nam nazwać odpowiednio kolumny:\n",
    "from pyspark.sql import Row\n",
    "spark.createDataFrame([Row(col1 = 1, col2 = 2), Row(col1 = 11, col2 = 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db95850f-2d59-4907-b9a7-395ff0720b07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Można korzystać również z źródeł JDBC lub innych dedykowanych bibliotek (MongoDB, Elastic, Cassandra i wiele więcej).\n",
    "\n",
    "\n",
    "Warto wspomnieć, że metoda `createDataFrame` umożliwia również tworzenie `DataFrame` na bazie `RDD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84bf89a8-e3d7-46ca-849b-a1da787f5c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "my_rdd = spark.sparkContext.parallelize([Row(col1 = 1, col2 = 2), Row(col1 = 11, col2 = 22)])\n",
    "df = spark.createDataFrame(my_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7b1fec-45fe-4d13-948e-ad0f353d811e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Można oczywiście dokonać konwersji w drugą stronę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cdbae1-41c8-4eec-8c4d-9bf5c99419c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9140a42-2e07-4212-86ba-6ad355771661",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "\n",
    "Utwórzmy DataFrame'y *uam_orders* i *uam_categories* analogicznie jak *uam_offers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e441b5-2647-4ac8-a376-e33f3b4930d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855ef77e-167f-4e78-bf15-3d635dd31bba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d2d665-2931-4a3b-a91a-53531c664a34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_orders = spark.table(\"uam_orders\")\n",
    "uam_categories = spark.table(\"uam_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589da542-8c2e-4424-a495-b1015f58d961",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Jak utworzyć DF za pomocą metody `createDataFrame` tak, aby zostały nadane domyślne nazwy atrybutów? W jakiej są postaci? Czy nazwy tych atrybutów są czytelne?\n",
    "\n",
    "Podpowiedź: dokumentację funkcji można sprawdzić dodając `?` na jej końcu (np. `spark.createDataFrame?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f924a21c-a0e2-4705-936b-cb02932dbeb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a21a7a-31f5-497d-8fc2-87c4858dbb0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1667f05-6c17-42a5-82df-e16529b38964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame([(1, 2), (11, 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bae874-261d-4230-badf-81703e94c00f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Odczyt metadanych\n",
    "\n",
    "DF posiada następujące metody i atrybuty umożliwiające eksplorację metadanych:\n",
    "1. `printSchema()` - wyświetla schemat danych \n",
    "2. `describe(*cols)` - zwraca DF ze statystykami kolumn\n",
    "3. `dtypes` - zwraca listę par nazwa kolumny, typ kolumny\n",
    "\n",
    "Klasa `SparkSession` (w naszym przypadku zmienna `spark`) zawiera atrybut `catalog`:\n",
    "\n",
    "spark.catalog - zwraca obiekt pozwalającym eksplorować metadane schematów i tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407d97d5-02ef-476a-a31e-743930179b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f81497-567f-4574-bfc0-78d90fa63eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.describe('offer_id', 'seller_id').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30da29fe-537c-4eed-b310-240cb91fdb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9f4c80-0138-444d-aa96-173e8036e002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f9ff5c-80ee-46ab-925e-90840353397d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465ec9c2-9cdf-4da6-9ea5-d16187647d7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listColumns('uam_categories', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9198843-adae-4d20-b36e-6a03c9795b38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W notebookach Databricks możemy również łatwo zmienić interpreter na SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d33932d-3e66-4a58-aa41-88dc4d619b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d256152-b95f-4923-a8b9-6a69beb1f67d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SELECT\n",
    "\n",
    "Należy zwrócić uwagę, która funkcja zwraca kolumny z przekształcanego DataFrame'a oraz nowe, a która zwraca tylko modyfikowane lub wylistowane kolumny.\n",
    "\n",
    "1. `uam_categories.select('*')` - wybór wszystkich kolumn (pytanie: czy ma to sens?)\n",
    "2. `uam_categories.select('category_id', 'category_level1')`  - wybór podzbioru kolumn\n",
    "3. `uam_categories.select(uam_categories.category_id)` - jak wyżej\n",
    "4. `uam_categories.select(uam_categories.category_id.alias('id'))` - aliasowanie kolumn\n",
    "5. `uam_categories.selectExpr('category_id as id', '2*1 as const')` - wyrażenie SQL’owe w klauzuli SELECT\n",
    "6. `from pyspark.sql.functions import lit; uam_categories.select(lit(2).alias('const'))` - przykład funkcji lit - generowanie stałych\n",
    "7. `uam_categories.withColumn('const', lit(2))` - dodawanie nowych kolumn\n",
    "8. `uam_categories.withColumnRenamed('category_id', 'id')` - aliasowanie kolumn - sposób nr 2\n",
    "9. `uam_categories.drop('category_level3')` - usuwanie kolumn\n",
    "\n",
    "Niektóre funkcje mogą wymagać przekazania argumentu jako kolumny. Może to być problematyczne np. po operacji złączenia. Rozwiązaniem jest funkcja `col`:\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import col\n",
    "uam_categories.select(col('category_level1'))\n",
    "```\n",
    "Funkcja `col` przyda się w późniejszych przykładach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9999d6ec-eb03-4e43-91b5-91381e9bbfc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zauważmy, że powyższe metody transformują `DataFrame` na inny `DataFrame` bez zwracania wyników (tzw. transformacja). \n",
    "\n",
    "Poniższe metody natomiast powodują zwrócenie wyników do `driver`'a (tzw. akcja):\n",
    "1. `show(n=20, truncate=True, vertical=False)` - wyświetla wyniki na ekran (`n` - liczba rekordów, `truncate` - czy zawijać wiersze, `vertical` - jeśli `True`, to każda kolumna jest wyświetlana w osobny wierszu)\n",
    "2. `collect()` - zwraca listę `Row`\n",
    "3. `display(input)` - działa podobnie jak `show`, ale wyniki są wyświetlane w sformatowanej tabeli \n",
    "\n",
    "Istnieje oczywiście szereg akcji powodujących zapis danych - umówimy je później"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224f85c0-5071-45a1-b069-b5ff5ad0de07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Samodzielne ćwiczenia\n",
    "\n",
    "Przetestujmy różne polecenia związane z odczytem danych i ich wyświetlaniem poniżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b7cab2-de3a-43cc-bebf-f40e8ae8cc70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykładowo:\n",
    "uam_categories.select('*').limit(5).show()\n",
    "uam_categories.select(uam_categories.category_id).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d53f897-d768-40f9-9e95-554f92633852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_categories.select('*').limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdfaf22-14a9-4164-993e-2a5797f4da5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  uam_categories.select('*').limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7c5a9d-bc13-49da-bec7-c8e8521b0de8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Funkcje wbudowane\n",
    "\n",
    "Funkcje są podobne (tj. nazwy, argumenty, działanie) jak w przypadku Spark SQL \n",
    "\n",
    "Przykłady znajdują się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22532742-466a-49a7-b45f-aec68b1add45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tworzymy DF z jedną kolumną i wierszem\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import Row\n",
    "df = spark.createDataFrame([Row(col='X')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f401f74-e82d-4929-901f-8d8645518175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funkcje operujące na datach\n",
    "display(\n",
    "    df.select(\n",
    "        current_timestamp(),\n",
    "        current_date(),\n",
    "        add_months(lit(\"2021-01-01\"), 1),\n",
    "        date_add(current_date(), 12),\n",
    "        datediff(current_date(), lit(\"2021-01-01\")),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e8622c-00f7-497e-9571-f17f47134102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  df.select(lit('1').cast('int'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d925bfc-fbc3-4fcc-a12b-8f1cadb353db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Należy zwracać uwagę, kiedy funkcja wymaga podania kolumny albo argumentu innego typu. \n",
    "\n",
    "Funkcja `expr` jest podobna do funkcji `selectExpr(col*)`. Obydwie umożliwiają do korzystania z funkcji w sposób taki jak korzysta się z nich w Spark SQL. Dobrym przykładem jest `reflect`, którego brakuje w module `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b8e569-5125-4723-bf7c-6cca4d8bc18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(expr(\"reflect('java.util.Currency', 'getAvailableCurrencies')\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c191b3-fe3f-4fac-8de9-4a5f46d2b19b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przejdźmy do bardziej skomplikowanych zastosowań - spróbujemy sparsować dane w formacie JSON, które znajdują się w kolumnie typu String:\n",
    "\n",
    "Utwórzmy DF z jednym rekordem  i kolumną o nazwie *json*. Korzystając z funkcji *from_json* utworzymy \n",
    "z kolumny zawierającej tekst kolumnę sparsowaną (tj. zgodną z podanym schematem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6a81f7-ee49-4334-9bb3-d7aa624df928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "simple_json = \"\"\"\n",
    "{\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"firstItemPrice\": {\n",
    "        \"amount\": \"6.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"firstItemPrice\": {\n",
    "        \"amount\": \"8.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "simple_json_df = spark.createDataFrame([Row(json=simple_json)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d1064c-376e-4af2-83e9-84314b7609f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tworzymy schemat dla ceny:\n",
    "price_schema = StructType(\n",
    "    [StructField(\"amount\", DecimalType(12, 2)), StructField(\"currency\", StringType())]\n",
    ")\n",
    "\n",
    "# schemat dla ceny pojedynczej pozycji\n",
    "single_item_schema = StructType([StructField(\"firstItemPrice\", price_schema)])\n",
    "\n",
    "# schemat dla zbioru różnych pozycji\n",
    "schema = StructType([StructField(\"items\", ArrayType(single_item_schema))])\n",
    "\n",
    "# gdy mamy gotowy schemat możemy sparsować nasz rekord:\n",
    "simple_json_df.select(from_json(simple_json_df.json, schema).alias(\"parsed_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0fd27d1-82b2-4100-974d-b515d4975255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Możemy również wygenerować automatycznie schemat dla danych w formacie JSON. W tym celu musimy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7384de98-450a-420d-be80-940b67fdcc8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# przekształcić dane do postaci RDD[string]\n",
    "as_rdd = simple_json_df.rdd.map( lambda r: r[0])\n",
    "\n",
    "# skrzystać z metody json\n",
    "spark.read.json(as_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fdd9b73-30f8-4af4-8912-bd73c1313f20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Schemat możemy również wykorzystać tworząc DF przy wykorzystaniu metody `SparkSession.createDataFrame(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e524cc36-98db-4044-8be0-e5349cea635e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "  [StructField(\"StringField\", StringType()), \n",
    "   StructField(\"IntField\", IntegerType())])\n",
    "spark.createDataFrame([(\"val1\", 1), (\"val2\", 2)], schema).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f35d3dd-3158-46e6-8178-685b66434110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenia\n",
    "\n",
    "Dana jest zmienna tekstowa w formacie JSON (zob. poniżej). Utwórz DF z jednym rekordem i kolumną o nazwie json. Z cennika dostaw pobierz cenę dostawy jednej sztuki z dowolnej metody dostawy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f491647-a81e-46c3-8193-476e5b09dfd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json=\"\"\"\n",
    "{\n",
    "  \"eventTime\": \"2018-03-14T03:25:24.516Z\",\n",
    "  \"priceList\": {\n",
    "    \"shippingRates\": null,\n",
    "    \"items\": [\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"773167b1-feec-4ae9-b20f-1ed8ccb7b1ed\",\n",
    "          \"qeppoId\": 6\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"6.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 5,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"758fcd59-fbfa-4453-ae07-4800d72c2ca5\",\n",
    "          \"qeppoId\": 8\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"8.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 5,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"7203cb90-864c-4cda-bf08-dc883f0c78ad\",\n",
    "          \"qeppoId\": 9\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"10.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 10,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"845efe05-0c96-47c3-a8cb-aa4699c158ce\",\n",
    "          \"qeppoId\": 11\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"12.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 10,\n",
    "        \"shippingTime\": null\n",
    "      }\n",
    "    ],\n",
    "    \"location\": {\n",
    "      \"city\": \"Internet\",\n",
    "      \"state\": \"zachodniopomorskie\",\n",
    "      \"postcode\": \"00-000\",\n",
    "      \"country\": \"PL\"\n",
    "    },\n",
    "    \"sendingAbroad\": false,\n",
    "    \"estimatedShippingTime\": \"PT0S\",\n",
    "    \"additionalInfo\": \"\",\n",
    "    \"id\": \"5459a18b-4da0-45ef-a910-6472700bde06\",\n",
    "    \"lastModified\": \"2018-03-14T04:25:24.515+01:00\",\n",
    "    \"flags\": {\n",
    "      \"freeReturn\": false,\n",
    "      \"freeDelivery\": false,\n",
    "      \"useCostPerWeight\": false\n",
    "    },\n",
    "    \"weight\": null,\n",
    "    \"templateId\": null,\n",
    "    \"shippingCost\": {\n",
    "      \"lowest\": {\n",
    "        \"amount\": \"6.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      },\n",
    "      \"freeDelivery\": false\n",
    "    }\n",
    "  },\n",
    "  \"updatedPaths\": []\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a53e2c-084e-47ae-8495-1f73281a23e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#miejsce na rozwiązanie\n",
    "#podpowiedź: skorzystać z funkcji  `get_json_object`\n",
    "spark.sql(\"desc function get_json_object\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ed771c-7783-4a5c-995d-04a9776cba08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d657c12-8187-4da3-8ce3-aa030a67abc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df = spark.createDataFrame([Row(json=json)])\n",
    "display(\n",
    "    df.select(get_json_object(df.json, \"$.priceList.items[0].firstItemPrice.amount\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a135ecc7-3ae4-41a7-b4f9-00d6a2f2c9db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Wyświetl liczbę atrybutów dla kilku przykładowych ofert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0831bd-3e71-48a8-9b62-ed9231e78981",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# miejsce na rozwiązanie\n",
    "# podpowiedź: skorzystać z funkcji  `size`\n",
    "display(spark.sql(\"desc function size\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9073b44-3059-4e86-b442-dcc9b8c60b35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4293e54-dc14-4a78-901b-b110719041ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.select(uam_offers.offer_id,  size(uam_offers.attributes)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2e1ca4-0048-4cf5-bd06-ace835f1554b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przykład `explode`\n",
    "\n",
    "Znajdź wszystkie oferty (identyfikator, nazwę oraz typ). Każdy typ powinien się znaleźć w nowym wierszu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebb291b-f363-423e-9d6a-7ed3cec8626b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "display(\n",
    "    uam_offers.select(\n",
    "        explode(uam_offers.types).alias(\"types_\"),\n",
    "        uam_offers.offer_id,\n",
    "        uam_offers.offer_name,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16aefe5-d6c4-4ce8-92aa-94988087f0a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TODO - tu skończyłem\n",
    "TODO - popraw wszystkie `show`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510c5d2e-b148-4a93-abee-2da5b14d6303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## User Defined Function (UDF)\n",
    "\n",
    "W przypadku, gdy nie ma możliwości skorzystania z funkcji wbudowanych możemy rejestrować własne. Gdy będziemy je wykorzystywać w Spark SQL możemy to zrobić na dwa sposoby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5a160f-bc64-4855-b92d-c853cf6c631e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bez określenia zwracanego typu:\n",
    "def total(price, item_quantity):\n",
    "    return price * item_quantity\n",
    "\n",
    "spark.udf.register(\"total_udf\", total)\n",
    "spark.sql(\"select total_udf(1, 2)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a160f6-50ec-4e37-bf2e-4712ed1dae6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# z określeniem zwracanego typu:\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "spark.udf.register(\"total_udf_typed\", total, DecimalType(12, 2))\n",
    "spark.sql(\"select total_udf_typed(1, 2)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a13b2eb-d30c-4d77-947f-429e76d77a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przykładowe zapytania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c3348b-0476-4094-a8cb-9a9a8b22b00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_query = \"select total_udf(unit_price, quantity) from uam_orders\"\n",
    "spark.sql(total_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c751263-4b23-4144-a6b4-8880104db071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_typed_query = \"select total_udf_typed( cast(unit_price as decimal(12, 2)), cast(quantity as decimal(12, 2))) from uam_orders\"\n",
    "spark.sql(total_typed_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538f757b-fba1-42db-b26f-adea28a4b77d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ale to nie zadziała zgodnie z oczekiwaniem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afd176d2-91fc-496f-8988-a05fe082f345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_typed_query2 = \"select total_udf_typed(unit_price,  quantity), price, quantity from uam_orders\"\n",
    "spark.sql(total_typed_query2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c76e9a7-8c3f-4e00-a440-be95fcb659a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Różnicę widać dopiero po wykonaniu `printSchema`\n",
    "\n",
    "Dla poprawnego wywołania zadziałała niejawna konwersja double->decimal (dot. *quantity*)\n",
    "\n",
    "W przypadku zapytania, gdzie wynik jest niezgodny z oczekiwanym, mamy mnożenie dwóch liczb typu `double`. Oczekujemy, że funkcja zwróci `decimal`, ale zwraca `double`, co jest konwertowane na wartość `null`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8875cf-0d62-4bfe-8c0c-c4f7fbe6dfa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Jeśli korzystamy z DataFrame API, to mamy następujące możliwości w kwestii UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06764e53-0615-41e2-800a-97282f62d7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 \n",
    "from pyspark.sql.functions import udf\n",
    "def total(price, item_quantity):\n",
    "    return price * item_quantity\n",
    "\n",
    "total_typed_udf = udf(total, DecimalType(12,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8637a629-8d4b-4c59-9b41-8097932a59fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 dekorator udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('decimal(12,2)')\n",
    "def total_typed_udf(price, item_quantity):\n",
    "    return price * item_quantity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac0e74f-ee45-490b-bc9b-ca4b9a7f1ba3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W obydwu przypadkach możemy pominąć typ zwracany przez UDF:\n",
    "\n",
    "`total_udf = udf(total)`\n",
    "\n",
    "Lub w wersji z dekoratorem:\n",
    "\n",
    "```\n",
    "@udf\n",
    "def total_udf(price, item_quantity):\n",
    "    return price * item_quantity * 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4220ee-787b-48e4-9ad6-c89f1bc1879c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ostatecznie możemy skorzystać analogicznie jak z innych funkcji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8f67af-23ec-446f-a3df-c2e40de214e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_orders.select(total_typed_udf(uam_orders.price.cast('decimal(12,2)'), uam_orders.quantity.cast('decimal(12,2)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35b10dd-0c93-4217-a59f-103ee1f576a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "UDF - przykład\n",
    "\n",
    "Napisz funkcję, która dla 3 poziomów drzewa kategorii utworzy jedną nazwę. \n",
    "\n",
    "W przypadku, gdy kategoria na danym poziomie nie jest wypełniona należy użyć nazwy kategorii na wyższym poziomie. Jeśli kategoria w ogóle nie jest wypełniona, to można wpisać dowolny ciąg znaków. Przykłady:\n",
    "\n",
    "*Elektronika - RTV i AGD - RTV i AGD*\n",
    "\n",
    "*Elektronika - Elektronika - Elektronika*\n",
    "\n",
    "*Brak nazwy - Brak nazwy - Brak nazwy*\n",
    "\n",
    "*Kolekcje i sztuka - Kolekcje - Pocztówki *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e473ba-8a5c-48e7-9f81-ccb126947cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def format_category_tree(cat1,  cat2, cat3):\n",
    "    cat1_or_default = cat1 or \"Brak nazwy\"\n",
    "    cat2_or_default = cat2 or cat1_or_default\n",
    "    cat3_or_default = cat3 or cat2_or_default\n",
    "    return cat1_or_default + \" - \" + cat2_or_default + \" - \" + cat3_or_default\n",
    "\n",
    "uam_categories.select(format_category_tree(uam_categories.category_level1, uam_categories.category_level2, uam_categories.category_level3)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004cdbb8-64b9-4520-9798-17be6e070258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zadanie\n",
    "\n",
    "Zamień ciągi znaków w nazwach ofert tak aby były z wielkich liter:\n",
    "1) Skorzystaj z funkcji wbudowanych\n",
    "2) Stwórz UDF'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56a1219-9096-4968-8f50-7ec60010a1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "uam_offers.select(upper(uam_offers.offer_name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b7f6dd-43bd-4603-9ce3-046859feedfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf\n",
    "def to_upper(s: str):\n",
    "  return s.upper()\n",
    "\n",
    "uam_offers.select(to_upper(uam_offers.offer_name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed451e9d-932d-4371-a068-67d3226e57b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "UDF a wydajność:\n",
    "\n",
    "UDF w PySpark są wolniejsze niż w Scali oraz Javie ze względu na konieczność serializacji i deserializacji danych przy wykonywaniu funkcji pythonowych (dwukrotnie). \n",
    "https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/\n",
    "\n",
    "Od wersji 2.3 wprowadzono tzw. Pandas UDF, zaś począwszy od wersji 3.0 wprowadzone nowe uporządkowane API:\n",
    "\n",
    "https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\n",
    "\n",
    "Przykład znajduje się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "136ba6e0-8a3e-43bf-b3cf-bd48064105d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"decimal(12,2)\")\n",
    "def total_typed_udf(price: pd.Series, item_quantity: pd.Series) -> pd.Series:\n",
    "    return price * item_quantity\n",
    "  \n",
    "uam_orders.select(total_typed_udf(uam_orders.price.cast('decimal(12,2)'), uam_orders.quantity.cast('decimal(12,2)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a5132a-e5dc-4995-8f6c-be0f1b11c7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WHERE\n",
    "\n",
    "Działanie metody `where` jest bardzo intuicyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76087ec4-41b8-4ad8-9485-5dcf43de2e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy transakcje dokonane po 2018-01-01 23:00:00\n",
    "uam_orders.where(uam_orders.buyingTime > '2018-01-01 23:00:00').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d3baec-343b-4fb5-b5c4-5a2ef7523fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy oferty z jedną odsłoną\n",
    "uam_offers.where(uam_offers.pv == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d672ae-f3a5-4c95-af61-75cb92206e62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Intuicyjne również są operatory logiczne, chociaż warto zwrócić uwagę, na potrzebę umieszczania wyrażeń w nawiasach ze względu na kolejność wykonywania operatorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a495bf-735c-4259-b513-f5f33dbd7a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Koniunkcja\n",
    "\n",
    "# Znajdźmy kategorie, które na 3. poziomie mają wartość różną od NULL, a na 1. \"Moda i uroda\"\n",
    "uam_categories.where((uam_categories.category_level3.isNotNull()) & (uam_categories.category_level1 == 'Moda i uroda')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20385256-c86c-41fb-bd9d-bdef9b6a0fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternatywa\n",
    "\n",
    "# Znajdźmy oferty z jedną odsłoną lub liczbą odsłon większą niż 10\n",
    "uam_offers.where((uam_offers.pv == 1) | (uam_offers.pv > 10)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f660f5-81cd-4b94-a8b0-de3d51cb6e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Istnieje wiele funkcji dostępnych na kolumnach, które zwracają typ `boolean`. Np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d2af49-71a3-457e-8039-ce2331aab239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy kategorie na 1. poziomie, które nie składają się ze słów rozdzielonych spójnikiem “i”\n",
    "uam_categories.where(~uam_categories.category_level1.like('% i %')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab8fb05-e6e7-4b4c-b626-6363fe49d29b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Kolejnym przykładem takich funkcji są `isNull()` oraz `isNotNull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8484c6-668c-4baf-8451-6e6572f4106c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_orders.where(uam_orders.userAgent.isNull()).show()\n",
    "uam_orders.where(uam_orders.userAgent.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39eaeb56-5db6-4c70-af6b-54b2c2e7913a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Intuicyjne są również operatory porównania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d656f2b6-8a22-432f-8635-7763286e99a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.where(uam_offers.pv > 1)\n",
    "uam_offers.where(uam_offers.pv >= 1)\n",
    "uam_offers.where(uam_offers.pv < 1)\n",
    "uam_offers.where(uam_offers.pv <= 1)\n",
    "uam_offers.where(uam_offers.pv != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78879e7d-1ff8-4f2e-816d-02aa0f6bb476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przykład\n",
    "\n",
    "Znajdźmy identyfikatory ofert wraz z dostępnymi rozmiarami. Informacje  o rozmiarach znajdują się w kolumnie *attributes*. Rozmiar jest jednym z rodzajów atrybutów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967c7c3c-ca22-4811-bd9d-1434b9cb3dde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "(\n",
    "  uam_offers.\n",
    "   select(uam_offers.offer_id, explode(uam_offers.attributes).alias('attribute')).\n",
    "   where(col('attribute.name') == 'rozmiar').\n",
    "   select(uam_offers.offer_id, explode(col('attribute.values')).alias('attribute_value')).\n",
    "   show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de3a95c5-58c0-4a67-982f-f945c6d24dfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zadanie \n",
    "\n",
    "Znajdźmy oferty \"Kup teraz\" (typ zawiera BUY_NOW), które są niedostępne (tj. stan magazynowy *stock_current_quantity* jest równy 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deabc03-3152-44ad-b55d-147aee3754dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e234e999-417a-48a5-8fe2-ec437f7f7aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d36e3824-18db-4300-bea2-a2f569e7ab93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "uam_offers.where((array_contains(uam_offers.types, 'BUY_NOW')) & (uam_offers.stock_current_quantity == 0)).select('offer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83878b8b-49ef-465b-9722-809708a0ada9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "uam_offers.withColumn(\"type\", explode(uam_offers.types)).where((col(\"type\") == 'BUY_NOW') & (uam_offers.stock_current_quantity == 0)).select('offer_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5945491c-2540-45a0-b5e1-923e366c4f92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# JOIN\n",
    "\n",
    "Do operacji złączenia służy metoda `join` zdefiniowana na DataFrame:\n",
    "\n",
    "`join(other, on=None, how=None)`\n",
    "\n",
    "Znaczenie parametrów:\n",
    "1. `other` - DF, z którym przeprowadzamy operację złączenia (prawa strona)\n",
    "2. `on` - string, lista kolumn lub wyrażenie bazujące na kolumnach\n",
    "3. `how` -  *inner* (domyślnie), *cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9f25993-ebc2-4801-8177-eca9b6cb6588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przykład \n",
    "\n",
    "Połącz oferty z kategoriami. Wybierz tylko te oferty, które są droższe niż 100 zł i należą do kategorii \"Kolekcje i sztuka\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090b9a3d-a084-42c0-951b-3f781885ea6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.join(uam_categories, uam_offers.category_leaf == uam_categories.category_id).\\\n",
    "where((uam_offers.buynow_price > 100) & (uam_categories.category_level1 == 'Kolekcje i sztuka')). \\\n",
    "select(uam_offers.offer_id, uam_offers.offer_name, uam_categories.category_level1, uam_categories.category_level2, uam_categories.category_level3).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b30010-d85c-4a93-a1f4-e4c09ab6d2ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Sporządź zestawienie ofert sprzedanych danego dnia (identyfikator, nazwa). Wybierz tylko te oferty, które są droższe niż 1000 zł."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b850fc1-f901-4033-b69f-59709ce29139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63db1b46-e70c-4ab8-8fba-9b0a0119438d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3885254d-7562-4932-87cb-85d672ae6007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.join(uam_orders, uam_offers.offer_id == uam_orders.offer_id).\\\n",
    "where(uam_offers.buynow_price > 1000).\\\n",
    "select(uam_offers.offer_id, uam_offers.offer_name). \\\n",
    "show()\n",
    "\n",
    "uam_offers.join(uam_orders, 'offer_id').\\\n",
    "where(uam_offers.buynow_price > 1000).\\\n",
    "select(uam_offers.offer_id, uam_offers.offer_name).\\\n",
    "show()\n",
    "\n",
    "uam_offers.join(uam_orders, ['offer_id']).\\\n",
    "where(uam_offers.buynow_price > 1000).\\\n",
    "select(uam_offers.offer_id, uam_offers.offer_name).\\\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9106549-77e2-4bef-86ea-a0eedf826529",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Grupowanie i agregacja\n",
    "\n",
    "Odpowiednikiem klauzuli `GROUP BY` jest DataFrame'owa metoda `groupBy(*cols)`, natomiast do wyliczania agregatów służy metoda `agg(*exprs)` zdefiniowana w klasie `GroupedData`. \n",
    "\n",
    "Dla hipotycznego DataFrame'a *df* kombinacja wywołań w/w metod będzie następująca:\n",
    "\n",
    "`df.groupBy(*cols).agg(*exprs)`, gdzie\n",
    "\n",
    "1. `*cols` - lista kolumn lub string (nazwy kolumn)\n",
    "2. `*exprs` - lista wyrażeń grupujących kolumny lub słownik, gdzie kluczem jest nazwa kolumny a wartością funkcja grupująca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47ebe034-01bd-46d7-911a-984411c194d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Przykład\n",
    "\n",
    "Znajdź maksymalną i minimalną cenę ofert w każdej z kategorii na 1. poziomie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0a8316f-070e-4a6a-8fb6-a286d35630b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf).\\\n",
    "groupBy(uam_categories.category_level1). \\\n",
    "agg(min(uam_offers.buynow_price.cast('double')).alias('min_price'), max(uam_offers.buynow_price.cast('double')).alias('max_price')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "745e5701-b2b4-4d43-afed-4aaebe424a27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#inne rozwiązania:\n",
    "\n",
    "# lista kolumn w groupBy\n",
    "uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf).\\\n",
    "groupBy([uam_categories.category_level1]).\\\n",
    "agg(min(uam_offers.buynow_price.cast('double')), max(uam_offers.buynow_price.cast('double'))).\\\n",
    "show()\n",
    "\n",
    "# string w groupBy\n",
    "uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf).\\\n",
    "groupBy('uam_categories.category_level1').\\\n",
    "agg(min(uam_offers.buynow_price.cast('double')), max(uam_offers.buynow_price.cast('double'))).\\\n",
    "show()\n",
    "\n",
    " # lista string'ów w groupBy \n",
    "uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf).\\\n",
    "groupBy(['uam_categories.category_level1']).\\\n",
    "agg(min(uam_offers.buynow_price.cast('double')), max(uam_offers.buynow_price.cast('double'))).\\\n",
    "show()\n",
    "\n",
    "# słownik w agg\n",
    "uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf).\\\n",
    "withColumn('buynow_price_as_double1', uam_offers.buynow_price.cast('double')). \\\n",
    "withColumn('buynow_price_as_double2', uam_offers.buynow_price.cast('double')). \\\n",
    "groupBy('uam_categories.category_level1').\\\n",
    "agg({'buynow_price_as_double1': 'min', 'buynow_price_as_double2': 'max'}).\\\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0558924e-62fc-45ae-9c96-df9e1ba939d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Funkcje grupujące nazywają się podobnie jak w Spark SQL (różniące się zostały pogrubione):\n",
    "\n",
    "*avg*\n",
    "\n",
    "*collect_list*\n",
    "\n",
    "*collect_set*\n",
    "\n",
    "*corr*\n",
    "\n",
    "*first*\n",
    "\n",
    "*kurtosis*\n",
    "\n",
    "*last*\n",
    "\n",
    "*max*\n",
    "\n",
    "*mean*\n",
    "\n",
    "*min*\n",
    "\n",
    "*skewness*\n",
    "\n",
    "*stddev_pop*\n",
    "\n",
    "*stddev_samp*\n",
    "\n",
    "*stddev*\n",
    "\n",
    "*var_pop*\n",
    "\n",
    "*var_samp*\n",
    "\n",
    "*variance*\n",
    "\n",
    "**approx_count_distinct**\n",
    "\n",
    "*count*\n",
    "\n",
    "**countDistinct**\n",
    "\n",
    "*sum*\n",
    "\n",
    "**sumDistinct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c5e24a6-64f6-40ad-a29d-9b67b98d622b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Niektóre funkcje grupujące są również metodami `pyspark.sql.group.GroupedData` (tj. pomijamy wywołanie `agg(...)`), np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef0018e-6c39-4e64-a7b3-498864cd663d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf = uam_offers.groupBy(uam_offers.duration)\n",
    "gdf.max('pv', 'stock_initial_quantity')\n",
    "# Pozostałe funkcje:\n",
    "gdf.avg('pv', 'stock_initial_quantity')\n",
    "gdf.count()\n",
    "gdf.mean('pv', 'stock_initial_quantity')\n",
    "gdf.min('pv', 'stock_initial_quantity')\n",
    "gdf.sum('pv', 'stock_initial_quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a58da3ad-ed72-43e6-b0bc-0772ff711b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Funkcja `pivot` umożliwia zamianę wartości w wierszach wybranej kolumny na nazwę nowych kolumn oraz przeprowadzenie odpowiedniej agregacji. Przykładowo, jeśli chcemy pogrupować sumę odsłon oferty w przekroju kategorii (1. poziom) oraz czasu trwania, tak aby czas trwania był nowych kolumnach, to musimy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06c76d58-0a7f-4df8-836a-204ff8fcbc6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(df.category_level1, df.duration).sum('pv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "166fb8ef-5a08-4442-9e44-37dde970c8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf)\n",
    "df.groupBy(df.category_level1).pivot('duration', values=['PT168H', 'PT240H']).sum('pv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d059f8a-b3c9-4e23-bcc7-cbab3f17b5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Uwaga: funkcja pivot przyjmuje opcjonalny argument `values`. Jeśli jest wypełniony, to zostaną utworzone te kolumny, które znajdują się na liście. \n",
    "Pytanie: która wersja pivot jest wydajniejsza? Dlaczego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de88e86d-c807-4b8a-ac55-43615adbe1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Grupowanie (`cube/rollup`) oraz sortowanie - przykład\n",
    "\n",
    "Znajdźmy sumę odsłon oraz liczbę ofert w podziale na kategorię (1. poziom) oraz czas trwania oferty. Posortujmy wyniki ze względu na poziom grupowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ed02d7-1986-4d87-9cb5-9aed766a63dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = uam_offers.join(uam_categories, uam_categories.category_id == uam_offers.category_leaf)\n",
    "\n",
    "from pyspark.sql.functions import grouping_id, grouping, sum, count, col\n",
    "\n",
    "df.cube('category_level1', 'duration').\\\n",
    "agg(grouping_id(), grouping('category_level1'), grouping('duration'), \\\n",
    "sum(df.pv), count('*')).\\\n",
    "orderBy(col('grouping_id()')).show(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df64540-9091-462a-8d7e-db568778211c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W odróżnieniu od Spark SQL nie ma odpowiednika funkcji `grouping sets`\n",
    "\n",
    "Dodatkowe funkcje `grouping_id()` oraz `grouping(col)` służą odpowiednio do określania poziomu agregacji oraz określenia, czy wartość w kolumnie jest wynikiem agregacji, czy jest wartością występującą w zbiorze wynikowym (jest to szczególnie ważne w przypadku wartości *NULL*)\n",
    "\n",
    "Funkcja `rollup` w odróżnieniu od `cube` nie generuje wszystkich możliwych podsumowań, ale takie które wynikają z kolejności podanych kolumn. Np. `rollup(col1, col2)` zwróci całościowe podsumowanie oraz na poziomie kolumny *col1* oraz pary *(col1, col2)*. Nie zostanie wygenerowane podsumowanie na poziomie *col2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd073fb-6d3e-41a8-bc53-8e324a024f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = uam_offers.join(uam_orders, uam_orders.offer_id == uam_offers.offer_id, \"left_anti\")\n",
    "\n",
    "display(\n",
    "  df.groupBy().agg(max(df.buynow_price.cast(\"double\").alias(\"max_price\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92921f3c-a0ad-40c1-b316-e611cb8aaf3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Funkcje analityczne \n",
    "\n",
    "Przeanalizujmy działanie funkcji analitycznych na przykładzie:\n",
    "\n",
    "Znajdźmy kategorię (1. poziom), w której użytkownik dokonał 1. transakcji (jako kupujący)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75aa001-5a1a-4801-aa72-dbf256d575a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "df = uam_orders.\\\n",
    "join(uam_offers, 'offer_id').\\\n",
    "join(uam_categories, uam_categories.category_id == uam_offers.category_leaf)\n",
    "\n",
    "window = Window.partitionBy(df.buyer_id).orderBy(df.buyingTime)\n",
    "\n",
    "df.withColumn('rank_', rank().over(window)).\\\n",
    "where(col('rank_') == 1).\\\n",
    "select(df.buyer_id, df.category_level1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86bea8c4-ac3d-436a-88aa-0b0fcf8fe664",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A teraz trochę teorii dla uporządkowania:\n",
    "\n",
    "Funkcje analityczne używamy analogicznie jak inne funkcje. Jedyną różnicą jest to, że po nazwie funkcji analitycznej wywołujemy metodę `over(window_spec)`, gdzie `window_spec` jest definicją okna.\n",
    "\n",
    "`pyspark.sql.Window` posiada następujące metody:\n",
    "1. `partitionBy(cols*)` - określenie kolumn, które wchodzą w skład definicji partycji\n",
    "2. `orderBy(cols*)` - określenie sortowania wewnątrz partycji\n",
    "3. `rangeBetween(start, end)` - definicja okna względem wartości danego wiersza\n",
    "4. `rowsBetween(start, end)` - definicja okna względem pozycji danego wiersza\n",
    "\n",
    "Stałe użyte w metodach `rangeBetween`, `rowsBetween` oznaczają: \n",
    "1. `Window.currentRow` -  bieżący wiersz\n",
    "2. `Window.unboundedPreceding` - wszystkie wiersze poprzedzające bieżący wiersz\n",
    "3. `Window.unboundedFollowing` - wszystkie wiersze następujące po bieżącym wierszu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f006bd8e-d3bd-44c1-8d61-02f1d71f5d92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Wybrane funkcje analityczne:\n",
    "\n",
    "```\n",
    "cume_dist()\n",
    "dense_rank()\n",
    "lag(col, count=1, default=None)\n",
    "last(col, ignorenulls=False)\n",
    "lead(col, count=1, default=None)\n",
    "ntile(n)\n",
    "percent_rank()\n",
    "row_number()\n",
    "rank()\n",
    "```\n",
    "\n",
    "Funkcje agregujące z definicją okna (np. `sum`, `max`, `count` itd.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "addbafc6-7322-4b9f-97ee-700ac31ce05e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Operacje na zbiorach \n",
    "\n",
    "Działania na zbiorach najlepiej zobrazują poniższe przykłady:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d130bd32-78b7-48ae-b0b7-47d0674ff974",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tworzymy następujące DF’y:\n",
    "df1 = spark.range(0, 10, 2) # liczby parzyste\n",
    "df2 = spark.range(10) # liczby naturalne\n",
    "\n",
    "print(\"Suma zbiorów (może zawierać duplikaty)\")\n",
    "df1.union(df2).show()\n",
    "print(\"Suma zbiorów (bez duplikatów)\" )\n",
    "df1.union(df2).distinct().show()\n",
    "print(\"Część wspólna zbiorów\")\n",
    "df2.intersect(df1).show()\n",
    "print(\"Różnica zbiorów\")\n",
    "df2.subtract(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4951e9-ba12-462e-b007-1be546a3a1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Znajdź oferty, które nie znalazły nabywcy (identyfikatory) korzystając z operacji na zbiorach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ca4d6c-029a-45e4-9bd9-91e1f801e6a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce133a9f-97fe-4c93-a0c1-f7fb1528bf56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99e5567d-6114-49f3-99de-b318c1d471ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.select(uam_offers.offer_id).subtract(uam_orders.select(uam_orders.offer_id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1bb3ba6-2646-4d92-8e59-05675b55432a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Operacja `cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da8dee0-9fe8-42db-92e6-285862ad5a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metoda `cache`/ `persist` służy do przechowywania danych w pamięci operacyjnej. Dzięki temu możemy uniknąć wykonywania tych samych akcji wielokrotnie. W PySparku możemy używać `cache` na poziomie dowolnego DataFrame'a w odróżnieniu do Spark SQL, gdzie działaliśmy na poziomie tabeli. Przejdźmy do przykładów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16661e0c-9dc7-42ea-8885-726d3b43fe97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aby wyczyścić wszystkie dane z pamięci podręcznej możemy wykonać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ba9d04-dbef-4351-8b36-63c451705caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4244b5-a61c-4d60-b36a-c2811936b026",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Utwórzmy DataFrame'a, który zawiera oferty będące aukcjami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d2659b-b0e4-4153-8104-486ee72ec7f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "auctions_stats_df = uam_offers.where(array_contains(col(\"types\"), \"AUCTION\")).groupBy(col(\"duration\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79aadc65-6194-4f42-8560-bed7c301624d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(auctions_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33b4b11-97ba-4ac8-a23a-0665a3590af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Przeanalizujemy plan wykonania przed `cache`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57509c9d-43d0-4742-9458-472f9dad53cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "auctions_stats_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62cfbd72-df3e-401c-a9e0-23d5cea5f33f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Następnie po operacji cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa92df4-b7b5-41b4-aab9-5ce5c64cd542",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cached_df = auctions_stats_df.cache()\n",
    "display(cached_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4632dcf7-e0e8-4fce-af71-5b5ec2c2160d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Zauważmy, że optimalizator Sparka jest w stanie stwierdzić, które dane są w cache'u na poziomie analizy zapytania. W tym przypadku użyliśmy DataFrame'a, na którym nie wykonano operacji `cache`, ale mimo wszystko optymalizator znalazł odpowiednie dane w pamięci podręcznej i je wykorzystał:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ead03c-55fc-4e27-aa7d-07257c3f00ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(auctions_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f979ad7e-8192-4c31-b1c6-469f353068d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Po usunięciu danych z pamięci podręcznej plan zapytania wygląda tak jak wcześniej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00fadd18-b440-4453-b5eb-37f12da18a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uncached_df = cached_df.unpersist()\n",
    "display(uncached_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfbfc15e-ec8a-48ca-9e4f-64e2317162ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "W związku z tym, że Spark korzysta z zaawansowanych mechanizmów takich jak:\n",
    "  * CBO - Cost Based Optimizer (optymalizator kosztowy)\n",
    "  * AQE - Adaptive Query Execution (adaptacyjne wykonanie zapytania)\n",
    "  * Delta cache - pamięć podręczna dla plików w formacie Delta\n",
    "\n",
    "plan zapytania może się różnić w zależności od wolumenów danych i parametryzacji Spark'a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a09a007-4fea-45ae-bd93-9aec4fd98063",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DataFrame jako widok tymczasowy\n",
    "\n",
    "A co jeśli odpowiada nam bardziej SQL API? Możemy zarejestrować DF jako widok tymczasowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a463bea-b29a-4e67-9ad5-863ad86290b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unsoldOffersDf = uam_offers.select(uam_offers.offer_id).subtract(uam_orders.select(uam_orders.offer_id))\n",
    "unsoldOffersDf.createOrReplaceTempView ('unsoldOffers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3d8459-f1a3-424e-8bce-e7fdf4df9dd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(1) from unsoldOffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f7e2d8-4f85-49bd-95fa-9d3fa118d6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1) from unsoldOffers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d907f08d-841f-4d01-934e-8c836a833f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ec9217-cb9a-4156-8785-09be176cf7aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Może to być bardzo wygodne przy wykonywaniu zapytań z różnych źródeł (np. HDFS/DBFS i JDBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8230d30-2b83-4b40-a64f-264cb85fee91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Zapis danych\n",
    "\n",
    "Do zapisu danych służy klasa `pyspark.sql.DataFrameWriter`, która jest tworzona poprzez wywołanie metody `write()` klasy `DataFrame`. Najważniejsze metody to:\n",
    "\n",
    "`saveAsTable(name, format=None, mode=None, partitionBy=None, **options)`\n",
    "1. `name` - nazwa tabeli\n",
    "2. `mode` - tryb zapisu: \n",
    "\n",
    " *append*: dodaje nowe wiersze\n",
    " \n",
    " *overwrite*: nadpisuje dane\n",
    " \n",
    " *ignore*: jeśli dane istnieją w tabeli nie robi nic\n",
    " \n",
    " *error* (domyślnie): zwraca wyjątek, jeśli dane istnieją\n",
    " \n",
    "3. *partitionBy* - nazwy kolumn z partycjami\n",
    "4. `**options` - inne opcje\n",
    "\n",
    "`insertInto(tableName, overwrite=False)`\n",
    "1. `tableName` - nazwa tabeli\n",
    "2. `overwrite` - czy nadpisywać dane\n",
    "\n",
    "Przeanalizujmy następujące przykłady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1920cde-c214-4961-9e14-baa33f0cb464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "rm -r /user/hive/warehouse/uam_categories_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "003b4c7a-03f7-4adf-a2f0-889941513795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists default.uam_categories_sample\")\n",
    "\n",
    "# zapis danych do nowej tabeli\n",
    "uam_categories.sample(fraction=0.1).\\\n",
    "write.saveAsTable('default.uam_categories_sample', format='orc')\n",
    "# Dodanie nowych rekordów do tabeli\n",
    "uam_categories.sample(fraction=0.1).write.insertInto('default.uam_categories_sample')\n",
    "\n",
    "# Zapis danych do tabeli partycjonowanej\n",
    "uam_categories.sample(fraction=0.1).\\\n",
    "write.saveAsTable('default.uam_categories_sample', format='orc', mode='overwrite', partitionedBy=['category_level1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b77843-308f-461e-90ce-4bf7b18cb440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dane możemy również zapisywać bezpośrednio do plików:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31e969b6-fc51-4aa4-a42e-63d0bc6e5a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.sample(False, 0.01).write.\\\n",
    "partitionBy('duration').\\\n",
    "mode('overwrite').\\\n",
    "parquet(path='dbfs:/tmp/uam_offers_sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9793689b-b05f-4030-9e6f-641fef80f99b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analogicznie zapis może być dokonany do formatów: *orc, csv, text, json* oraz do źródeł danych *JDBC* (metoda `jdbc`)\n",
    "\n",
    "W przypadku, gdy mamy zdefiniowaną tabelę w metastore, ale zdecydowaliśmy się pisać bezpośrednio do plików (jak w powyższym przykładzie), to dane mogę nie być widoczne. Poniższe polecenie sprawi, że metastore odświeży swoje metadane i uwzględni nowe pliki:\n",
    "```python\n",
    "spark.catalog.recoverPartitions(nazwa_tabeli)\n",
    "```\n",
    "Powyższe polecenie jest odpowiednikiem SQL'owego `MSCK REPAIR TABLE ...`\n",
    "<br><br><br>\n",
    "Czasami może się zdarzyć, że odczytujemy dane z pewnej tabeli, potem ją modyfikujemy (albo robi inny program), a potem znowu próbujemy ją odczytać. W przypadku, gdy spark zwróci wyjątek należy wykonać:\n",
    "```python\n",
    "spark.catalog.refreshTable(nazwa_tabeli)\n",
    "```\n",
    "Jest to odpowiednik SQL'owego `REFRESH TABLE ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f39a230-29f7-423c-a082-2f81ce2b0359",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pandas & Koalas\n",
    "\n",
    "Biblioteka pandas jest jedną z najpolularniejszych bibliotek pythonowych do analizy danych (zob.: https://pandas.pydata.org/). Aby z niech korzystać wystarczy wywołać metodą `toPandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc9cdd3-72f1-492a-a649-f4d9cef04116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_categories.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b7b0cd-1e95-4a28-a9aa-e8058aac5cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Warto pamiętać że `toPandas` jest akcją. Oznacza to, że wszystkie dane są zbierane do pamięcie drivera (działa podobnie jak `collect`). Dlatego powstała biblioteka koalas, która od wersji Spark 3.2 będzie już częścią Spark'a. Zapewnia ono podobne API jak pandas, ale przetwarzania mają charakter rozproszony. Oznacza to, że nie ogranicza nas pamięć driver'a :)\n",
    "\n",
    "Przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67d48e2-f582-45e8-82e1-86a7f278a8ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to było potrzebne przed spark 3.2\n",
    "# import databricks.koalas as ks\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "koalasDf = uam_categories.to_koalas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345d5070-4408-404b-834c-f10d302902d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Porównanie różnych API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a892b9f1-9d56-4e48-a15e-a5f33fde10af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) RDD - imperatywne API\n",
    "uam_categories.rdd.filter(lambda r: r.category_level1 == 'Elektronika' ).map(lambda r: (r.category_level2, 1)).reduceByKey(lambda v1, v2: v1+v2).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07879559-7e4c-4eea-9caa-199123ac6498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Spark SQL - deklaratywne API\n",
    "spark.sql(\"select category_level2,  count(1) from uam_categories where category_level1 = 'Elektronika' group by category_level2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063a7357-ec82-472d-adde-61cd754cc09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) DataFrame - deklaratywne API\n",
    "uam_categories.where(uam_categories.category_level1 == 'Elektronika').groupBy('category_level2').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f36ec2-f054-403b-af95-0bf2973cbd22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 981540389271916,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "PySpark - workshop",
   "notebookOrigID": 1037057353578201,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
