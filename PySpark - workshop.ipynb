{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481836e5-41df-4436-8863-2790eeada5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Wprowadzenie\n",
    "\n",
    "W notebook'ach zdefiniowana jest zmienna `spark` typu `SparkSession`, która będzie punktem startowym naszej pracy ze Sparkiem. Tego typu konwencja nazewnicza jest bardzo częsta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1af3aa-ee2f-48dc-a0b2-487c14d441b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27deb15c-d62b-43d4-b1ad-d462b6d7787e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Jak tworzymy Dataframe?\n",
    "Pierwszym sposobem jest użycie Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9ed29c-098a-4528-9723-5271d063003d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers = spark.sql(\"select * from uam_offers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54093dca-d39a-43e7-9e7e-a545971477e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W powyższym przypadku prościej będzie jednak skorzystać z metody `table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8f0423-3572-4215-a1c1-121ed5a57bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers = spark.table(\"uam_offers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e99776-5472-48ef-a237-68352e06c190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Metoda `sql` przyjmuje dowolne zapytanie Spark SQL. Np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54a4965-3a99-444c-8792-1e7fdf028820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show create table uam_offers\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fecc0d-aa51-4056-8c74-25a989e0527f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Możemy również odczytać dane bezpośrednio z plików:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66483e43-dd4f-4639-bab5-619464596f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location = \"dbfs:/user/hive/warehouse/uam_offers\"\n",
    "uam_offers_from_file = spark.read.format(\"delta\").load(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac8dba5-9076-4af9-8d94-3c9baec56b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drugi sposób wymaga od nas znajomości formatu pliku (w typ przypadku `delta`) oraz ścieżki.\n",
    "\n",
    "\n",
    "Kolejnym sposobem jest wykorzystanie metody `createDataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95b4a8d-bb8f-426c-bc6a-67717e2a712d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wersja 1: \n",
    "# Pierwszy argument: lista tupli\n",
    "# Drugi (opcjonalny) argument: lista string (nazwy kolumn)\n",
    "spark.createDataFrame([(1, 2), (11, 22)], ['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1061cbda-9d68-4388-a36c-eddce6583fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wersja 2:\n",
    "# Wykorzystujemy klasę Row, która pozwala nam nazwać odpowiednio kolumny:\n",
    "from pyspark.sql import Row\n",
    "spark.createDataFrame([Row(col1 = 1, col2 = 2), Row(col1 = 11, col2 = 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db95850f-2d59-4907-b9a7-395ff0720b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Można korzystać również z źródeł JDBC lub innych dedykowanych bibliotek (MongoDB, Elastic, Cassandra i wiele więcej).\n",
    "\n",
    "\n",
    "Warto wspomnieć, że metoda `createDataFrame` umożliwia również tworzenie `DataFrame` na bazie `RDD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84bf89a8-e3d7-46ca-849b-a1da787f5c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "my_rdd = spark.sparkContext.parallelize([Row(col1 = 1, col2 = 2), Row(col1 = 11, col2 = 22)])\n",
    "df = spark.createDataFrame(my_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7b1fec-45fe-4d13-948e-ad0f353d811e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Można oczywiście dokonać konwersji w drugą stronę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cdbae1-41c8-4eec-8c4d-9bf5c99419c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baaa39f8-354f-4517-9683-67dd2138b69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Kolejnym sposobem jest przekazanie listy słowników:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b9f19d-2d79-4da0-b479-17f2ad5e55c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "  [\n",
    "    {\"col1\": 1, \"col2\": 2},\n",
    "    {\"col1\": 11, \"col2\": 22},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9140a42-2e07-4212-86ba-6ad355771661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "\n",
    "Utwórzmy DataFrame'y *uam_orders* i *uam_categories* analogicznie jak *uam_offers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e441b5-2647-4ac8-a376-e33f3b4930d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855ef77e-167f-4e78-bf15-3d635dd31bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d2d665-2931-4a3b-a91a-53531c664a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uam_orders = spark.table(\"uam_orders\")\n",
    "uam_categories = spark.table(\"uam_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589da542-8c2e-4424-a495-b1015f58d961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Jak utworzyć DF za pomocą metody `createDataFrame` tak, aby zostały nadane domyślne nazwy atrybutów? W jakiej są postaci? Czy nazwy tych atrybutów są czytelne?\n",
    "\n",
    "Podpowiedź: dokumentację funkcji można sprawdzić dodając `?` na jej końcu (np. `spark.createDataFrame?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f924a21c-a0e2-4705-936b-cb02932dbeb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a21a7a-31f5-497d-8fc2-87c4858dbb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1667f05-6c17-42a5-82df-e16529b38964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame([(1, 2), (11, 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bae874-261d-4230-badf-81703e94c00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Odczyt metadanych\n",
    "\n",
    "DF posiada następujące metody i atrybuty umożliwiające eksplorację metadanych:\n",
    "1. `printSchema()` - wyświetla schemat danych \n",
    "2. `schema` - zwraca schemat danych (reprezentacja Spark'owa)\n",
    "3. `describe(*cols)` - zwraca DF ze statystykami kolumn\n",
    "4. `dtypes` - zwraca listę par nazwa kolumny, typ kolumny\n",
    "\n",
    "Klasa `SparkSession` (w naszym przypadku zmienna `spark`) zawiera atrybut `catalog`:\n",
    "\n",
    "spark.catalog - zwraca obiekt pozwalającym eksplorować metadane schematów i tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407d97d5-02ef-476a-a31e-743930179b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193a4c1c-c483-4fcd-8401-5d23d266d8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f81497-567f-4574-bfc0-78d90fa63eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.describe('offer_id', 'seller_id').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30da29fe-537c-4eed-b310-240cb91fdb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9f4c80-0138-444d-aa96-173e8036e002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f9ff5c-80ee-46ab-925e-90840353397d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465ec9c2-9cdf-4da6-9ea5-d16187647d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listColumns('default.uam_categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9198843-adae-4d20-b36e-6a03c9795b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W notebookach Databricks możemy również łatwo zmienić interpreter na SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d33932d-3e66-4a58-aa41-88dc4d619b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d256152-b95f-4923-a8b9-6a69beb1f67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SELECT\n",
    "\n",
    "Należy zwrócić uwagę, która funkcja zwraca kolumny z przekształcanego DataFrame'a oraz nowe, a która zwraca tylko modyfikowane lub wylistowane kolumny.\n",
    "\n",
    "1. `uam_categories.select('*')` - wybór wszystkich kolumn (pytanie: czy ma to sens?)\n",
    "2. `uam_categories.select('category_id', 'category_level1')`  - wybór podzbioru kolumn\n",
    "3. `uam_categories.select(uam_categories.category_id)` - jak wyżej\n",
    "4. `uam_categories.select(uam_categories.category_id.alias('id'))` - aliasowanie kolumn\n",
    "5. `uam_categories.selectExpr('category_id as id', '2*1 as const')` - wyrażenie SQL’owe w klauzuli SELECT\n",
    "6. `from pyspark.sql.functions import lit; uam_categories.select(lit(2).alias('const'))` - przykład funkcji lit - generowanie stałych\n",
    "7. `uam_categories.withColumn('const', lit(2))` - dodawanie nowych kolumn\n",
    "8. `uam_categories.withColumnRenamed('category_id', 'id')` - aliasowanie kolumn - sposób nr 2\n",
    "9. `uam_categories.drop('category_level3')` - usuwanie kolumn\n",
    "\n",
    "Niektóre funkcje mogą wymagać przekazania argumentu jako kolumny. Może to być problematyczne np. po operacji złączenia. Rozwiązaniem jest funkcja `col`:\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import col\n",
    "uam_categories.select(col('category_level1'))\n",
    "```\n",
    "Funkcja `col` przyda się w późniejszych przykładach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9999d6ec-eb03-4e43-91b5-91381e9bbfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zauważmy, że powyższe metody transformują `DataFrame` na inny `DataFrame` bez zwracania wyników (tzw. transformacja). \n",
    "\n",
    "Poniższe metody natomiast powodują zwrócenie wyników do `driver`'a (tzw. akcja):\n",
    "1. `show(n=20, truncate=True, vertical=False)` - wyświetla wyniki na ekran (`n` - liczba rekordów, `truncate` - czy zawijać wiersze, `vertical` - jeśli `True`, to każda kolumna jest wyświetlana w osobny wierszu)\n",
    "2. `collect()` - zwraca listę `Row`\n",
    "3. `display(input)` - działa podobnie jak `show`, ale wyniki są wyświetlane w sformatowanej tabeli \n",
    "\n",
    "Istnieje oczywiście szereg akcji powodujących zapis danych - umówimy je później"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224f85c0-5071-45a1-b069-b5ff5ad0de07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Samodzielne ćwiczenia\n",
    "\n",
    "Przetestujmy różne polecenia związane z odczytem danych i ich wyświetlaniem poniżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b7cab2-de3a-43cc-bebf-f40e8ae8cc70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykładowo:\n",
    "uam_categories.select('*').limit(5).show()\n",
    "uam_categories.select(uam_categories.category_id).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d53f897-d768-40f9-9e95-554f92633852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_categories.select('*').limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdfaf22-14a9-4164-993e-2a5797f4da5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  uam_categories.select('*').limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7c5a9d-bc13-49da-bec7-c8e8521b0de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funkcje wbudowane\n",
    "\n",
    "UWAGA: Funkcje są podobne jak w przypadku Spark SQL (tj. nazwy, argumenty, działanie)\n",
    "\n",
    "Docs:\n",
    "1. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "Przykłady znajdują się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22532742-466a-49a7-b45f-aec68b1add45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tworzymy DF z jedną kolumną i wierszem\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import Row\n",
    "df = spark.createDataFrame([Row(col='X')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f401f74-e82d-4929-901f-8d8645518175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funkcje operujące na datach\n",
    "display(\n",
    "    df.select(\n",
    "        F.current_timestamp(),\n",
    "        F.current_date(),\n",
    "        F.add_months(F.lit(\"2021-01-01\"), 1),\n",
    "        F.date_add(F.current_date(), 12),\n",
    "        F.datediff(F.current_date(), F.lit(\"2021-01-01\")),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e8622c-00f7-497e-9571-f17f47134102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  df.select(F.lit('1').cast('int'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d925bfc-fbc3-4fcc-a12b-8f1cadb353db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Należy zwracać uwagę, kiedy funkcja wymaga podania kolumny albo argumentu innego typu. \n",
    "\n",
    "Funkcja `expr` jest podobna do funkcji `selectExpr(col*)`. Obydwie umożliwiają do korzystania z funkcji w sposób taki jak korzysta się z nich w Spark SQL. Dobrym przykładem jest `reflect`, którego brakuje w module `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b8e569-5125-4723-bf7c-6cca4d8bc18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(F.expr(\"reflect('java.util.Currency', 'getAvailableCurrencies')\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c191b3-fe3f-4fac-8de9-4a5f46d2b19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przejdźmy do bardziej skomplikowanych zastosowań - spróbujemy sparsować dane w formacie JSON, które znajdują się w kolumnie typu String:\n",
    "\n",
    "Utwórzmy DF z jednym rekordem  i kolumną o nazwie *json*. Korzystając z funkcji *from_json* utworzymy \n",
    "z kolumny zawierającej tekst kolumnę sparsowaną (tj. zgodną z podanym schematem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6a81f7-ee49-4334-9bb3-d7aa624df928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "simple_json = \"\"\"\n",
    "{\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"firstItemPrice\": {\n",
    "        \"amount\": \"6.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"firstItemPrice\": {\n",
    "        \"amount\": \"8.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "simple_json_df = spark.createDataFrame([Row(json=simple_json)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "388a742e-b535-425f-823e-07fe39a42e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Możemy mozolnie utworzyć schemat jak w poniższym przykładzie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d1064c-376e-4af2-83e9-84314b7609f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tworzymy schemat dla ceny:\n",
    "price_schema = T.StructType(\n",
    "    [T.StructField(\"amount\", T.DecimalType(12, 2)), T.StructField(\"currency\", T.StringType())]\n",
    ")\n",
    "\n",
    "# schemat dla ceny pojedynczej pozycji\n",
    "single_item_schema = T.StructType([T.StructField(\"firstItemPrice\", price_schema)])\n",
    "\n",
    "# schemat dla zbioru różnych pozycji\n",
    "schema = T.StructType([T.StructField(\"items\", T.ArrayType(single_item_schema))])\n",
    "\n",
    "# gdy mamy gotowy schemat możemy sparsować nasz rekord:\n",
    "simple_json_df.select(from_json(simple_json_df.json, schema).alias(\"parsed_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0fd27d1-82b2-4100-974d-b515d4975255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ale nie jest to rekomendowane podejście!\n",
    "\n",
    "Możemy również wygenerować automatycznie schemat dla danych w formacie JSON. W tym celu musimy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7384de98-450a-420d-be80-940b67fdcc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json\n",
    "json_schema = simple_json_df.select(schema_of_json(simple_json).alias(\"schema\")).head()\n",
    "json_schema.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be587724-3335-4d88-840d-952d457f0d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_json_df.select(from_json(simple_json_df.json, json_schema.schema).alias(\"parsed_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb79a1e-b2c2-4407-a89f-9b3b7562f8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wspieranych jest kilka funkcji prefiksowanych `schema_of_...`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07282f6d-ac46-4069-a5a6-57bbba6cc7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[entry for entry in F.__dict__.keys() if entry.startswith(\"schema_of\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fdd9b73-30f8-4af4-8912-bd73c1313f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schemat możemy również wykorzystać tworząc DF przy wykorzystaniu metody `SparkSession.createDataFrame(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e524cc36-98db-4044-8be0-e5349cea635e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = T.StructType(\n",
    "  [T.StructField(\"StringField\", T.StringType()), \n",
    "   T.StructField(\"IntField\", T.IntegerType())])\n",
    "spark.createDataFrame([(\"val1\", 1), (\"val2\", 2)], schema).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f35d3dd-3158-46e6-8178-685b66434110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenia\n",
    "\n",
    "Dana jest zmienna tekstowa w formacie JSON (zob. poniżej). Utwórz DF z jednym rekordem i kolumną o nazwie json. Z cennika dostaw pobierz cenę dostawy jednej sztuki z dowolnej metody dostawy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f491647-a81e-46c3-8193-476e5b09dfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json=\"\"\"\n",
    "{\n",
    "  \"eventTime\": \"2018-03-14T03:25:24.516Z\",\n",
    "  \"priceList\": {\n",
    "    \"shippingRates\": null,\n",
    "    \"items\": [\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"773167b1-feec-4ae9-b20f-1ed8ccb7b1ed\",\n",
    "          \"qeppoId\": 6\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"6.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 5,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"758fcd59-fbfa-4453-ae07-4800d72c2ca5\",\n",
    "          \"qeppoId\": 8\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"8.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 5,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"7203cb90-864c-4cda-bf08-dc883f0c78ad\",\n",
    "          \"qeppoId\": 9\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"10.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 10,\n",
    "        \"shippingTime\": null\n",
    "      },\n",
    "      {\n",
    "        \"deliveryMethod\": {\n",
    "          \"id\": \"845efe05-0c96-47c3-a8cb-aa4699c158ce\",\n",
    "          \"qeppoId\": 11\n",
    "        },\n",
    "        \"firstItemPrice\": {\n",
    "          \"amount\": \"12.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"nextItemPrice\": {\n",
    "          \"amount\": \"0.00\",\n",
    "          \"currency\": \"PLN\"\n",
    "        },\n",
    "        \"packageSize\": 10,\n",
    "        \"shippingTime\": null\n",
    "      }\n",
    "    ],\n",
    "    \"location\": {\n",
    "      \"city\": \"Internet\",\n",
    "      \"state\": \"zachodniopomorskie\",\n",
    "      \"postcode\": \"00-000\",\n",
    "      \"country\": \"PL\"\n",
    "    },\n",
    "    \"sendingAbroad\": false,\n",
    "    \"estimatedShippingTime\": \"PT0S\",\n",
    "    \"additionalInfo\": \"\",\n",
    "    \"id\": \"5459a18b-4da0-45ef-a910-6472700bde06\",\n",
    "    \"lastModified\": \"2018-03-14T04:25:24.515+01:00\",\n",
    "    \"flags\": {\n",
    "      \"freeReturn\": false,\n",
    "      \"freeDelivery\": false,\n",
    "      \"useCostPerWeight\": false\n",
    "    },\n",
    "    \"weight\": null,\n",
    "    \"templateId\": null,\n",
    "    \"shippingCost\": {\n",
    "      \"lowest\": {\n",
    "        \"amount\": \"6.00\",\n",
    "        \"currency\": \"PLN\"\n",
    "      },\n",
    "      \"freeDelivery\": false\n",
    "    }\n",
    "  },\n",
    "  \"updatedPaths\": []\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a53e2c-084e-47ae-8495-1f73281a23e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#miejsce na rozwiązanie\n",
    "#podpowiedź: skorzystać z funkcji  `get_json_object`\n",
    "spark.sql(\"desc function get_json_object\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ed771c-7783-4a5c-995d-04a9776cba08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d657c12-8187-4da3-8ce3-aa030a67abc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df = spark.createDataFrame([Row(json=json)])\n",
    "display(\n",
    "    df.select(get_json_object(df.json, \"$.priceList.items[0].firstItemPrice.amount\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a135ecc7-3ae4-41a7-b4f9-00d6a2f2c9db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Wyświetl liczbę atrybutów dla kilku przykładowych ofert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99af07c9-d3d3-4510-b8ef-b18b820d8528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8808193e-e838-483a-be49-d841fac59f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "F.size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0831bd-3e71-48a8-9b62-ed9231e78981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# miejsce na rozwiązanie\n",
    "# podpowiedź: skorzystać z funkcji  `size`\n",
    "display(spark.sql(\"desc function size\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9073b44-3059-4e86-b442-dcc9b8c60b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4293e54-dc14-4a78-901b-b110719041ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.select(uam_offers.offer_id,  F.size(uam_offers.attributes)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2e1ca4-0048-4cf5-bd06-ace835f1554b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład `explode`\n",
    "\n",
    "Znajdź wszystkie oferty (identyfikator, nazwę oraz typ). Każdy typ powinien się znaleźć w nowym wierszu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebb291b-f363-423e-9d6a-7ed3cec8626b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "display(\n",
    "    uam_offers.select(\n",
    "        explode(uam_offers.types).alias(\"types_\"),\n",
    "        uam_offers.offer_id,\n",
    "        uam_offers.offer_name,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5c4a59e-5633-4fb9-89b1-44c9b85f98fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozbicie elementów tablicy na wiersze jest naprostszym sposobem na przetwarzanie elementów. Powoduje to jednak wzrost liczby przetwarzanych wierszy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84203bc3-4a56-44a3-9ed3-c6b3b0b93e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład - `transform`\n",
    "\n",
    "Zamieńmy wielkość liter z wielkich na małe w elementach tablicy `types`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c477b7-f3cb-470c-96ad-72977e7a0246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  uam_offers.select(\n",
    "    F.transform(\"types\", F.lower).alias(\"lowercased_elements\"),\n",
    "    \"types\"\n",
    "  ).drop_duplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee15e71d-738b-431f-a54f-0ce0bfda5d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W wyrażeniu lambda możemy podawać wyłącznie funkcje z `pyspark.sql.functions` i UDF'y stworzone w Scali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27dc0492-263e-4e80-8e29-39f7d0f10cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład - `filter`\n",
    "\n",
    "Możemy również odfiltrować elementy tablicy. W poniższym przykładzie wyświetlimy zawartość `attributes` tylko do atrybutów, których nazwa (`name`) równa się \"stan\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f359a7b1-4160-41f4-a635-c2247f4c0f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "display(\n",
    "  uam_offers.select(\n",
    "    F.filter(\"attributes\", lambda attrib: attrib[\"name\"] == \"stan\").alias(\"stan\"),\n",
    "    \"attributes\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de28673c-b231-426a-ac09-7de7e8833147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Za pomocą `filter` wyciągnij wszystkie wartości atrybutów o nazwie `stan` jak w powyższym przykładzie. Następnie za pomocą `transform` wyciagnij wartości (`values`). \n",
    "\n",
    "Dla chętnych\n",
    "\n",
    "Skorzystaj z dodatkowych funkcji `flatten`, aby pozbyć się zagnieżdżonych tablic. Możesz również skorzystać z notacji `[]` do odwołania się do elementu tablicy (podobnie jak w listach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beacca46-05d4-42e8-9ac6-f9abb9055a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "display(\n",
    "  uam_offers.select(\n",
    "      F.flatten(\n",
    "        F.transform(\n",
    "          F.filter(\"attributes\", lambda attrib: attrib[\"name\"] == \"stan\").alias(\"stan\"),\n",
    "          lambda state: state[\"values\"]\n",
    "      )\n",
    "    )[0]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510c5d2e-b148-4a93-abee-2da5b14d6303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Defined Function (UDF)\n",
    "\n",
    "W przypadku, gdy nie ma możliwości skorzystania z funkcji wbudowanych możemy rejestrować własne. Gdy będziemy je wykorzystywać w Spark SQL możemy to zrobić na dwa sposoby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5a160f-bc64-4855-b92d-c853cf6c631e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bez określenia zwracanego typu:\n",
    "def total(price, item_quantity):\n",
    "    return price * item_quantity\n",
    "\n",
    "\n",
    "spark.udf.register(\"total_udf\", total)\n",
    "display(spark.sql(\"select total_udf(1, 2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a160f6-50ec-4e37-bf2e-4712ed1dae6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# z określeniem zwracanego typu:\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "spark.udf.register(\"total_udf_typed\", total, DecimalType(12, 2))\n",
    "display(spark.sql(\"select total_udf_typed(1.0, 2.0)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a13b2eb-d30c-4d77-947f-429e76d77a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykładowe zapytania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c3348b-0476-4094-a8cb-9a9a8b22b00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_query = \"select total_udf(unit_price, quantity) from uam_orders\"\n",
    "\n",
    "display(spark.sql(total_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c751263-4b23-4144-a6b4-8880104db071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_typed_query = \"\"\"\n",
    "select \n",
    "  total_udf_typed(\n",
    "      cast(unit_price as decimal(12, 2)),\n",
    "      cast(quantity as decimal(12, 2))\n",
    "    )\n",
    "from uam_orders\"\"\"\n",
    "display(spark.sql(total_typed_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538f757b-fba1-42db-b26f-adea28a4b77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ale to nie zadziała zgodnie z oczekiwaniem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd176d2-91fc-496f-8988-a05fe082f345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_typed_query2 = (\n",
    "    \"select total_udf_typed(unit_price,  quantity), price, quantity from uam_orders\"\n",
    ")\n",
    "display(spark.sql(total_typed_query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c76e9a7-8c3f-4e00-a440-be95fcb659a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Różnicę widać dopiero po wykonaniu `printSchema`\n",
    "\n",
    "Dla poprawnego wywołania zadziałała niejawna konwersja double->decimal (dot. *quantity*)\n",
    "\n",
    "W przypadku zapytania, gdzie wynik jest niezgodny z oczekiwanym, mamy mnożenie dwóch liczb typu `double`. Oczekujemy, że funkcja zwróci `decimal`, ale zwraca `double`, co jest konwertowane na wartość `null`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8875cf-0d62-4bfe-8c0c-c4f7fbe6dfa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Jeśli korzystamy z DataFrame API, to mamy następujące możliwości w kwestii UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06764e53-0615-41e2-800a-97282f62d7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 \n",
    "from pyspark.sql.functions import udf\n",
    "def total(price, item_quantity):\n",
    "    return price * item_quantity\n",
    "\n",
    "total_typed_udf = udf(total, DecimalType(12,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8637a629-8d4b-4c59-9b41-8097932a59fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 dekorator udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('decimal(12,2)')\n",
    "def total_typed_udf(price, item_quantity):\n",
    "    return price * item_quantity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac0e74f-ee45-490b-bc9b-ca4b9a7f1ba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W obydwu przypadkach możemy pominąć typ zwracany przez UDF:\n",
    "\n",
    "`total_udf = udf(total)`\n",
    "\n",
    "Lub w wersji z dekoratorem:\n",
    "\n",
    "```\n",
    "@udf\n",
    "def total_udf(price, item_quantity):\n",
    "    return price * item_quantity * 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4220ee-787b-48e4-9ad6-c89f1bc1879c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ostatecznie możemy skorzystać analogicznie jak z innych funkcji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8f67af-23ec-446f-a3df-c2e40de214e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_orders.select(\n",
    "        total_typed_udf(\n",
    "            uam_orders.price.cast(\"decimal(12,2)\"),\n",
    "            uam_orders.quantity.cast(\"decimal(12,2)\"),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35b10dd-0c93-4217-a59f-103ee1f576a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "UDF - przykład\n",
    "\n",
    "Napisz funkcję, która dla 3 poziomów drzewa kategorii utworzy jedną nazwę. \n",
    "\n",
    "W przypadku, gdy kategoria na danym poziomie nie jest wypełniona należy użyć nazwy kategorii na wyższym poziomie. Jeśli kategoria w ogóle nie jest wypełniona, to można wpisać dowolny ciąg znaków. Przykłady:\n",
    "\n",
    "*Elektronika - RTV i AGD - RTV i AGD*\n",
    "\n",
    "*Elektronika - Elektronika - Elektronika*\n",
    "\n",
    "*Brak nazwy - Brak nazwy - Brak nazwy*\n",
    "\n",
    "*Kolekcje i sztuka - Kolekcje - Pocztówki *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e473ba-8a5c-48e7-9f81-ccb126947cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def format_category_tree(cat1, cat2, cat3):\n",
    "    cat1_or_default = cat1 or \"Brak nazwy\"\n",
    "    cat2_or_default = cat2 or cat1_or_default\n",
    "    cat3_or_default = cat3 or cat2_or_default\n",
    "    return cat1_or_default + \" - \" + cat2_or_default + \" - \" + cat3_or_default\n",
    "\n",
    "\n",
    "display(\n",
    "    uam_categories.select(\n",
    "        format_category_tree(\n",
    "            uam_categories.category_level1,\n",
    "            uam_categories.category_level2,\n",
    "            uam_categories.category_level3,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004cdbb8-64b9-4520-9798-17be6e070258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zadanie\n",
    "\n",
    "Zamień ciągi znaków w nazwach ofert tak aby były z wielkich liter:\n",
    "1) Skorzystaj z funkcji wbudowanych\n",
    "2) Stwórz UDF'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56a1219-9096-4968-8f50-7ec60010a1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "display(uam_offers.select(upper(uam_offers.offer_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b7f6dd-43bd-4603-9ce3-046859feedfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf\n",
    "def to_upper(s: str):\n",
    "    return s.upper()\n",
    "\n",
    "\n",
    "display(uam_offers.select(to_upper(uam_offers.offer_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed451e9d-932d-4371-a068-67d3226e57b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "UDF a wydajność:\n",
    "\n",
    "UDF w PySpark są wolniejsze niż w Scali oraz Javie ze względu na konieczność serializacji i deserializacji danych przy wykonywaniu funkcji pythonowych (dwukrotnie). \n",
    "https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/\n",
    "\n",
    "Od wersji 2.3 wprowadzono tzw. Pandas UDF, zaś począwszy od wersji 3.0 wprowadzone nowe uporządkowane API:\n",
    "\n",
    "https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\n",
    "\n",
    "Przykład znajduje się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136ba6e0-8a3e-43bf-b3cf-bd48064105d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@pandas_udf(\"decimal(12,2)\")\n",
    "def total_typed_udf(price: pd.Series, item_quantity: pd.Series) -> pd.Series:\n",
    "    return price * item_quantity\n",
    "\n",
    "\n",
    "display(\n",
    "    uam_orders.select(\n",
    "        total_typed_udf(\n",
    "            uam_orders.price.cast(\"decimal(12,2)\"),\n",
    "            uam_orders.quantity.cast(\"decimal(12,2)\"),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1164d34f-04db-436b-aee2-288ae14af8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Od najnowszej wersji Spark'a (3.5) dostępne jest kolejne usprawnienie, które rozwiązuje problem z podwójną serializacją/deserializacją - *Arrow Optimized UDFs:* https://www.databricks.com/blog/arrow-optimized-python-udfs-apache-sparktm-35\n",
    "\n",
    "W tym przypadku wystarczy tylko:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6897654-f4c7-478f-be01-555b5b9b3a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(useArrow=True)\n",
    "def total_arrow_udf(price, item_quantity):\n",
    "    return price * item_quantity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322dab63-0f7f-47f2-8078-ac064221f999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_orders.select(\n",
    "        total_arrow_udf(\n",
    "            uam_orders.price.cast(\"decimal(12,2)\"),\n",
    "            uam_orders.quantity.cast(\"decimal(12,2)\"),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f500ac-a2f0-4d56-b9c8-cec7a435a2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Możemy sprawdzić plan zapytania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f46f2fb-bec6-4724-b134-749ed24bc35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_orders.select(\n",
    "        total_arrow_udf(\n",
    "            uam_orders.price.cast(\"decimal(12,2)\"),\n",
    "            uam_orders.quantity.cast(\"decimal(12,2)\"),\n",
    "        )\n",
    "    ).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a5132a-e5dc-4995-8f6c-be0f1b11c7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WHERE\n",
    "\n",
    "Działanie metody `where` jest bardzo intuicyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76087ec4-41b8-4ad8-9485-5dcf43de2e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy transakcje dokonane po 2018-01-01 23:00:00\n",
    "display(uam_orders.where(uam_orders.buyingTime > \"2018-01-01 23:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d3baec-343b-4fb5-b5c4-5a2ef7523fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy oferty z jedną odsłoną\n",
    "display(uam_offers.where(uam_offers.pv == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d672ae-f3a5-4c95-af61-75cb92206e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Intuicyjne również są operatory logiczne, chociaż warto zwrócić uwagę, na potrzebę umieszczania wyrażeń w nawiasach ze względu na kolejność wykonywania operatorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a495bf-735c-4259-b513-f5f33dbd7a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Koniunkcja\n",
    "\n",
    "# Znajdźmy kategorie, które na 3. poziomie mają wartość różną od NULL, a na 1. \"Moda i uroda\"\n",
    "display(\n",
    "    uam_categories.where(\n",
    "        (uam_categories.category_level3.isNotNull())\n",
    "        & (uam_categories.category_level1 == \"Moda i uroda\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20385256-c86c-41fb-bd9d-bdef9b6a0fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternatywa\n",
    "\n",
    "# Znajdźmy oferty z jedną odsłoną lub liczbą odsłon większą niż 10\n",
    "display(uam_offers.where((uam_offers.pv == 1) | (uam_offers.pv > 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f660f5-81cd-4b94-a8b0-de3d51cb6e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Istnieje wiele funkcji dostępnych na kolumnach, które zwracają typ `boolean`. Np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d2af49-71a3-457e-8039-ce2331aab239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdźmy kategorie na 1. poziomie, które nie składają się ze słów rozdzielonych spójnikiem “i”\n",
    "display(uam_categories.where(~uam_categories.category_level1.like(\"% i %\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab8fb05-e6e7-4b4c-b626-6363fe49d29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Kolejnym przykładem takich funkcji są `isNull()` oraz `isNotNull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb8484c6-668c-4baf-8451-6e6572f4106c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(uam_orders.where(uam_orders.userAgent.isNull()))\n",
    "display(uam_orders.where(uam_orders.userAgent.isNotNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39eaeb56-5db6-4c70-af6b-54b2c2e7913a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Intuicyjne są również operatory porównania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d656f2b6-8a22-432f-8635-7763286e99a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.where(uam_offers.pv > 1)\n",
    "uam_offers.where(uam_offers.pv >= 1)\n",
    "uam_offers.where(uam_offers.pv < 1)\n",
    "uam_offers.where(uam_offers.pv <= 1)\n",
    "uam_offers.where(uam_offers.pv != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78879e7d-1ff8-4f2e-816d-02aa0f6bb476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład\n",
    "\n",
    "Znajdźmy identyfikatory ofert wraz z dostępnymi rozmiarami. Informacje  o rozmiarach znajdują się w kolumnie *attributes*. Rozmiar jest jednym z rodzajów atrybutów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967c7c3c-ca22-4811-bd9d-1434b9cb3dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "display(\n",
    "    uam_offers.select(\n",
    "        uam_offers.offer_id,\n",
    "        explode(uam_offers.attributes).alias(\"attribute\")\n",
    "    )\n",
    "    .where(col(\"attribute.name\") == \"rozmiar\")\n",
    "    .select(\n",
    "        uam_offers.offer_id,\n",
    "        explode(col(\"attribute.values\")).alias(\"attribute_value\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de3a95c5-58c0-4a67-982f-f945c6d24dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zadanie \n",
    "\n",
    "Znajdźmy oferty \"Kup teraz\" (typ zawiera BUY_NOW), które są niedostępne (tj. stan magazynowy *stock_current_quantity* jest równy 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deabc03-3152-44ad-b55d-147aee3754dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e234e999-417a-48a5-8fe2-ec437f7f7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36e3824-18db-4300-bea2-a2f569e7ab93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "display(\n",
    "    uam_offers.where(\n",
    "        (array_contains(uam_offers.types, \"BUY_NOW\"))\n",
    "        & (uam_offers.stock_current_quantity == 0)\n",
    "    ).select(\"offer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83878b8b-49ef-465b-9722-809708a0ada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "display(\n",
    "    uam_offers.withColumn(\"type\", explode(uam_offers.types))\n",
    "    .where((col(\"type\") == \"BUY_NOW\") & (uam_offers.stock_current_quantity == 0))\n",
    "    .select(\"offer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441c676c-a11b-4fea-9d01-3bd4a5c31d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aliasem dla metody `where` jest `filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aea4338-47fb-4ce7-a8b9-a3af9b4ec9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_offers.withColumn(\"type\", explode(uam_offers.types))\n",
    "    .filter((col(\"type\") == \"BUY_NOW\") & (uam_offers.stock_current_quantity == 0))\n",
    "    .select(\"offer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d6dd7a-b328-4013-8434-43e5fd363cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Do metod `where`/`filter` możemy przekazywać warunki w postaci klauzul SQL, tj.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbb9d22-fafe-4cf9-9d2b-e874d68e28de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_offers.withColumn(\"type\", explode(uam_offers.types))\n",
    "    .where(\"type = 'BUY_NOW' and stock_current_quantity = 0\")\n",
    "    .select(\"offer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8fc523e-d0d6-4214-ba86-56b0b4de1268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wracamy do funkcji wyższego rzędu - uzupełnienie `transform` i `filter`\n",
    "\n",
    "\n",
    "Przykład - `exists`\n",
    "\n",
    "Możemy również odfiltrować elementy tablicy. W poniższym przykładzie ograniczymy zawartość `attributes` tylko do atrybutów, których nazwa (name) równa się \"stan\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e28a2f-d42d-48ca-83ca-be0d2a7078ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  uam_offers.where(\n",
    "    F.exists(\"attributes\", lambda attrib: attrib[\"name\"] == \"stan\").alias(\"stan\"),\n",
    "    \"attributes\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5945491c-2540-45a0-b5e1-923e366c4f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# JOIN\n",
    "\n",
    "Do operacji złączenia służy metoda `join` zdefiniowana na DataFrame:\n",
    "\n",
    "`join(other, on=None, how=None)`\n",
    "\n",
    "Znaczenie parametrów:\n",
    "1. `other` - DF, z którym przeprowadzamy operację złączenia (prawa strona)\n",
    "2. `on` - string, lista kolumn lub wyrażenie bazujące na kolumnach\n",
    "3. `how` -  *inner* (domyślnie), *cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9f25993-ebc2-4801-8177-eca9b6cb6588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład \n",
    "\n",
    "Połącz oferty z kategoriami. Wybierz tylko te oferty, które są droższe niż 100 zł i należą do kategorii \"Kolekcje i sztuka\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090b9a3d-a084-42c0-951b-3f781885ea6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_offers.category_leaf == uam_categories.category_id\n",
    "    )\n",
    "    .where(\n",
    "        (uam_offers.buynow_price > 100)\n",
    "        & (uam_categories.category_level1 == \"Kolekcje i sztuka\")\n",
    "    )\n",
    "    .select(\n",
    "        uam_offers.offer_id,\n",
    "        uam_offers.offer_name,\n",
    "        uam_categories.category_level1,\n",
    "        uam_categories.category_level2,\n",
    "        uam_categories.category_level3,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b30010-d85c-4a93-a1f4-e4c09ab6d2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Sporządź zestawienie ofert sprzedanych danego dnia (identyfikator, nazwa). Wybierz tylko te oferty, które są droższe niż 1000 zł."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b850fc1-f901-4033-b69f-59709ce29139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63db1b46-e70c-4ab8-8fba-9b0a0119438d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3885254d-7562-4932-87cb-85d672ae6007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_offers.join(uam_orders, uam_offers.offer_id == uam_orders.offer_id)\n",
    "    .where(uam_offers.buynow_price > 1000)\n",
    "    .select(uam_offers.offer_id, uam_offers.offer_name)\n",
    ")\n",
    "\n",
    "display(\n",
    "    uam_offers.join(uam_orders, \"offer_id\")\n",
    "    .where(uam_offers.buynow_price > 1000)\n",
    "    .select(uam_offers.offer_id, uam_offers.offer_name)\n",
    ")\n",
    "\n",
    "display(\n",
    "    uam_offers.join(uam_orders, [\"offer_id\"])\n",
    "    .where(uam_offers.buynow_price > 1000)\n",
    "    .select(uam_offers.offer_id, uam_offers.offer_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9106549-77e2-4bef-86ea-a0eedf826529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grupowanie i agregacja\n",
    "\n",
    "Odpowiednikiem klauzuli `GROUP BY` jest DataFrame'owa metoda `groupBy(*cols)`, natomiast do wyliczania agregatów służy metoda `agg(*exprs)` zdefiniowana w klasie `GroupedData`. \n",
    "\n",
    "Dla hipotycznego DataFrame'a *df* kombinacja wywołań w/w metod będzie następująca:\n",
    "\n",
    "`df.groupBy(*cols).agg(*exprs)`, gdzie\n",
    "\n",
    "1. `*cols` - lista kolumn lub string (nazwy kolumn)\n",
    "2. `*exprs` - lista wyrażeń grupujących kolumny lub słownik, gdzie kluczem jest nazwa kolumny a wartością funkcja grupująca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47ebe034-01bd-46d7-911a-984411c194d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przykład\n",
    "\n",
    "Znajdź maksymalną i minimalną cenę ofert w każdej z kategorii na 1. poziomie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a8316f-070e-4a6a-8fb6-a286d35630b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    "    )\n",
    "    .groupBy(uam_categories.category_level1)\n",
    "    .agg(\n",
    "        min(uam_offers.buynow_price.cast(\"double\")).alias(\"min_price\"),\n",
    "        max(uam_offers.buynow_price.cast(\"double\")).alias(\"max_price\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745e5701-b2b4-4d43-afed-4aaebe424a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inne rozwiązania:\n",
    "\n",
    "# lista kolumn w groupBy\n",
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    "    )\n",
    "    .groupBy([uam_categories.category_level1])\n",
    "    .agg(\n",
    "        min(uam_offers.buynow_price.cast(\"double\")),\n",
    "        max(uam_offers.buynow_price.cast(\"double\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# string w groupBy\n",
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    "    )\n",
    "    .groupBy(\"uam_categories.category_level1\")\n",
    "    .agg(\n",
    "        min(uam_offers.buynow_price.cast(\"double\")),\n",
    "        max(uam_offers.buynow_price.cast(\"double\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# lista string'ów w groupBy\n",
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    "    )\n",
    "    .groupBy([\"uam_categories.category_level1\"])\n",
    "    .agg(\n",
    "        min(uam_offers.buynow_price.cast(\"double\")),\n",
    "        max(uam_offers.buynow_price.cast(\"double\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# słownik w agg\n",
    "display(\n",
    "    uam_offers.join(\n",
    "        uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    "    )\n",
    "    .withColumn(\"buynow_price_as_double1\", uam_offers.buynow_price.cast(\"double\"))\n",
    "    .withColumn(\"buynow_price_as_double2\", uam_offers.buynow_price.cast(\"double\"))\n",
    "    .groupBy(\"uam_categories.category_level1\")\n",
    "    .agg({\"buynow_price_as_double1\": \"min\", \"buynow_price_as_double2\": \"max\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0558924e-62fc-45ae-9c96-df9e1ba939d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Funkcje grupujące nazywają się podobnie jak w Spark SQL (różniące się zostały pogrubione):\n",
    "\n",
    "*avg*\n",
    "\n",
    "*collect_list*\n",
    "\n",
    "*collect_set*\n",
    "\n",
    "*corr*\n",
    "\n",
    "*first*\n",
    "\n",
    "*kurtosis*\n",
    "\n",
    "*last*\n",
    "\n",
    "*max*\n",
    "\n",
    "*mean*\n",
    "\n",
    "*min*\n",
    "\n",
    "*skewness*\n",
    "\n",
    "*stddev_pop*\n",
    "\n",
    "*stddev_samp*\n",
    "\n",
    "*stddev*\n",
    "\n",
    "*var_pop*\n",
    "\n",
    "*var_samp*\n",
    "\n",
    "*variance*\n",
    "\n",
    "**approx_count_distinct**\n",
    "\n",
    "*count*\n",
    "\n",
    "**countDistinct**\n",
    "\n",
    "*sum*\n",
    "\n",
    "**sumDistinct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c5e24a6-64f6-40ad-a29d-9b67b98d622b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Niektóre funkcje grupujące są również metodami `pyspark.sql.group.GroupedData` (tj. pomijamy wywołanie `agg(...)`), np.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef0018e-6c39-4e64-a7b3-498864cd663d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf = uam_offers.groupBy(uam_offers.duration)\n",
    "gdf.max(\"pv\", \"stock_initial_quantity\")\n",
    "# Pozostałe funkcje:\n",
    "gdf.avg(\"pv\", \"stock_initial_quantity\")\n",
    "gdf.count()\n",
    "gdf.mean(\"pv\", \"stock_initial_quantity\")\n",
    "gdf.min(\"pv\", \"stock_initial_quantity\")\n",
    "gdf.sum(\"pv\", \"stock_initial_quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a58da3ad-ed72-43e6-b0bc-0772ff711b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Funkcja `pivot` umożliwia zamianę wartości w wierszach wybranej kolumny na nazwę nowych kolumn oraz przeprowadzenie odpowiedniej agregacji. Przykładowo, jeśli chcemy pogrupować sumę odsłon oferty w przekroju kategorii (1. poziom) oraz czasu trwania, tak aby czas trwania był nowych kolumnach, to musimy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166fb8ef-5a08-4442-9e44-37dde970c8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = uam_offers.join(\n",
    "    uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    ")\n",
    "display(\n",
    "    df.groupBy(df.category_level1)\n",
    "    .pivot(\"duration\", values=[\"PT168H\", \"PT240H\"])\n",
    "    .sum(\"pv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d059f8a-b3c9-4e23-bcc7-cbab3f17b5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uwaga: funkcja pivot przyjmuje opcjonalny argument `values`. Jeśli jest wypełniony, to zostaną utworzone te kolumny, które znajdują się na liście. \n",
    "Pytanie: która wersja pivot jest wydajniejsza? Dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af42d457-671c-4144-b372-7703f1bb6907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.groupBy(df.category_level1).pivot(\"duration\").sum(\"pv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de88e86d-c807-4b8a-ac55-43615adbe1b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Grupowanie (`cube/rollup`) oraz sortowanie - przykład\n",
    "\n",
    "Znajdźmy sumę odsłon oraz liczbę ofert w podziale na kategorię (1. poziom) oraz czas trwania oferty. Posortujmy wyniki ze względu na poziom grupowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ed02d7-1986-4d87-9cb5-9aed766a63dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import grouping_id, grouping, sum, count, col\n",
    "\n",
    "df = uam_offers.join(\n",
    "    uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    ")\n",
    "\n",
    "display(\n",
    "    df.cube(\"category_level1\", \"duration\")\n",
    "    .agg(\n",
    "        grouping_id(),\n",
    "        grouping(\"category_level1\"),\n",
    "        grouping(\"duration\"),\n",
    "        sum(df.pv),\n",
    "        count(\"*\"),\n",
    "    )\n",
    "    .orderBy(col(\"grouping_id()\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df64540-9091-462a-8d7e-db568778211c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W odróżnieniu od Spark SQL nie ma odpowiednika funkcji `grouping sets` (tzn. jest dostępna implementacja w Sparku 4.0 w wersji [preview](https://spark.apache.org/docs/4.0.0-preview2/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupingSets.html)).\n",
    "\n",
    "Dodatkowe funkcje `grouping_id()` oraz `grouping(col)` służą odpowiednio do określania poziomu agregacji oraz określenia, czy wartość w kolumnie jest wynikiem agregacji, czy jest wartością występującą w zbiorze wynikowym (jest to szczególnie ważne w przypadku wartości *NULL*)\n",
    "\n",
    "Funkcja `rollup` w odróżnieniu od `cube` nie generuje wszystkich możliwych podsumowań, ale takie które wynikają z kolejności podanych kolumn. Np. `rollup(col1, col2)` zwróci całościowe podsumowanie oraz na poziomie kolumny *col1* oraz pary *(col1, col2)*. Nie zostanie wygenerowane podsumowanie na poziomie *col2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92921f3c-a0ad-40c1-b316-e611cb8aaf3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funkcje analityczne \n",
    "\n",
    "Przeanalizujmy działanie funkcji analitycznych na przykładzie:\n",
    "\n",
    "Znajdźmy kategorię (1. poziom), w której użytkownik dokonał 1. transakcji (jako kupujący)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d75aa001-5a1a-4801-aa72-dbf256d575a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "df = uam_orders.join(uam_offers, \"offer_id\").join(\n",
    "    uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    ")\n",
    "\n",
    "window = Window.partitionBy(df.buyer_id).orderBy(df.buyingTime)\n",
    "\n",
    "display(\n",
    "    df.withColumn(\"rank_\", rank().over(window))\n",
    "    .where(col(\"rank_\") == 1)\n",
    "    .select(df.buyer_id, df.category_level1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86bea8c4-ac3d-436a-88aa-0b0fcf8fe664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A teraz trochę teorii dla uporządkowania:\n",
    "\n",
    "Funkcje analityczne używamy analogicznie jak inne funkcje. Jedyną różnicą jest to, że po nazwie funkcji analitycznej wywołujemy metodę `over(window_spec)`, gdzie `window_spec` jest definicją okna.\n",
    "\n",
    "`pyspark.sql.Window` posiada następujące metody:\n",
    "1. `partitionBy(cols*)` - określenie kolumn, które wchodzą w skład definicji partycji\n",
    "2. `orderBy(cols*)` - określenie sortowania wewnątrz partycji\n",
    "3. `rangeBetween(start, end)` - definicja okna względem wartości danego wiersza\n",
    "4. `rowsBetween(start, end)` - definicja okna względem pozycji danego wiersza\n",
    "\n",
    "Stałe użyte w metodach `rangeBetween`, `rowsBetween` oznaczają: \n",
    "1. `Window.currentRow` -  bieżący wiersz\n",
    "2. `Window.unboundedPreceding` - wszystkie wiersze poprzedzające bieżący wiersz\n",
    "3. `Window.unboundedFollowing` - wszystkie wiersze następujące po bieżącym wierszu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f006bd8e-d3bd-44c1-8d61-02f1d71f5d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wybrane funkcje analityczne:\n",
    "\n",
    "```\n",
    "cume_dist()\n",
    "dense_rank()\n",
    "lag(col, count=1, default=None)\n",
    "last(col, ignorenulls=False)\n",
    "lead(col, count=1, default=None)\n",
    "ntile(n)\n",
    "percent_rank()\n",
    "row_number()\n",
    "rank()\n",
    "```\n",
    "\n",
    "Funkcje agregujące z definicją okna (np. `sum`, `max`, `count` itd.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "addbafc6-7322-4b9f-97ee-700ac31ce05e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Operacje na zbiorach \n",
    "\n",
    "Działania na zbiorach najlepiej zobrazują poniższe przykłady:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d130bd32-78b7-48ae-b0b7-47d0674ff974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tworzymy następujące DF’y:\n",
    "df1 = spark.range(0, 10, 2)  # liczby parzyste\n",
    "df2 = spark.range(10)  # liczby naturalne\n",
    "\n",
    "print(\"Suma zbiorów (może zawierać duplikaty)\")\n",
    "df1.union(df2).show()\n",
    "print(\"Suma zbiorów (bez duplikatów)\")\n",
    "df1.union(df2).distinct().show()\n",
    "print(\"Część wspólna zbiorów\")\n",
    "df2.intersect(df1).show()\n",
    "print(\"Różnica zbiorów\")\n",
    "df2.subtract(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4951e9-ba12-462e-b007-1be546a3a1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Znajdź oferty, które nie znalazły nabywcy (identyfikatory) korzystając z operacji na zbiorach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ca4d6c-029a-45e4-9bd9-91e1f801e6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce133a9f-97fe-4c93-a0c1-f7fb1528bf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e5567d-6114-49f3-99de-b318c1d471ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    uam_offers.select(uam_offers.offer_id).subtract(\n",
    "        uam_orders.select(uam_orders.offer_id)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1bb3ba6-2646-4d92-8e59-05675b55432a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Operacja `cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da8dee0-9fe8-42db-92e6-285862ad5a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Metoda `cache`/ `persist` służy do przechowywania danych w pamięci operacyjnej. Dzięki temu możemy uniknąć wykonywania tych samych akcji wielokrotnie. W PySparku możemy używać `cache` na poziomie dowolnego DataFrame'a w odróżnieniu do Spark SQL, gdzie działaliśmy na poziomie tabeli. Przejdźmy do przykładów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16661e0c-9dc7-42ea-8885-726d3b43fe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aby wyczyścić wszystkie dane z pamięci podręcznej możemy wykonać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ba9d04-dbef-4351-8b36-63c451705caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4244b5-a61c-4d60-b36a-c2811936b026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Utwórzmy DataFrame'a, który zawiera oferty będące ofertami z działu \"Kolekcje i sztuka\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d2659b-b0e4-4153-8104-486ee72ec7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = uam_offers.join(\n",
    "    uam_categories, uam_offers.category_leaf == uam_categories.category_id\n",
    ").where(uam_categories.category_level1 == \"Kolekcje i sztuka\").select(\n",
    "  \"uam_offers.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79aadc65-6194-4f42-8560-bed7c301624d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33b4b11-97ba-4ac8-a23a-0665a3590af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sprawdźmy, czy nasz DataFrame jest w cache'u:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57509c9d-43d0-4742-9458-472f9dad53cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62cfbd72-df3e-401c-a9e0-23d5cea5f33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Następnie po operacji cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa92df4-b7b5-41b4-aab9-5ce5c64cd542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cached_df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6834fbeb-eaf5-475d-a588-09d1d8dd93d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cached_df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5cfd60c-a46b-4cf2-ad13-52f659b611d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cached_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4632dcf7-e0e8-4fce-af71-5b5ec2c2160d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zauważmy, że optimalizator Sparka jest w stanie stwierdzić, które dane są w cache'u na poziomie analizy zapytania. W tym przypadku użyliśmy DataFrame'a, na którym nie wykonano operacji `cache`, ale mimo wszystko optymalizator znalazł odpowiednie dane w pamięci podręcznej i jest w stanie je wykorzystać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda2fa54-bb6e-45ba-bdbf-5e3d49e59ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f979ad7e-8192-4c31-b1c6-469f353068d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Możemy usunąć dane z pamięci podręcznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fadd18-b440-4453-b5eb-37f12da18a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uncached_df = cached_df.unpersist()\n",
    "display(uncached_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a4b999-539b-4bab-bcb0-b35b853ba483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uncached_df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137b015c-d5ec-4df8-b592-68755bf3ca1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71f54c3-9476-47ea-abdc-e399a9afc4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cached_df.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfbfc15e-ec8a-48ca-9e4f-64e2317162ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W związku z tym, że Spark korzysta z zaawansowanych mechanizmów takich jak:\n",
    "  * CBO - Cost Based Optimizer (optymalizator kosztowy)\n",
    "  * AQE - Adaptive Query Execution (adaptacyjne wykonanie zapytania)\n",
    "  * Delta cache - pamięć podręczna dla plików w formacie Delta\n",
    "\n",
    "plan zapytania może się różnić w zależności od wolumenów danych i parametryzacji Spark'a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a09a007-4fea-45ae-bd93-9aec4fd98063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DataFrame jako widok tymczasowy\n",
    "\n",
    "A co jeśli odpowiada nam bardziej SQL API? Możemy zarejestrować DF jako widok tymczasowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a463bea-b29a-4e67-9ad5-863ad86290b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unsold_offers_df = uam_offers.select(uam_offers.offer_id).subtract(\n",
    "    uam_orders.select(uam_orders.offer_id)\n",
    ")\n",
    "unsold_offers_df.createOrReplaceTempView(\"unsold_offers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948f88d0-53af-493b-89d3-d5a6cd8c17d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3d8459-f1a3-424e-8bce-e7fdf4df9dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "  count(1)\n",
    "from\n",
    "  unsold_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f7e2d8-4f85-49bd-95fa-9d3fa118d6ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select count(*) from unsold_offers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ec9217-cb9a-4156-8785-09be176cf7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Może to być bardzo wygodne przy wykonywaniu zapytań z różnych źródeł (np. HDFS/DBFS i JDBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8230d30-2b83-4b40-a64f-264cb85fee91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Zapis danych\n",
    "\n",
    "Do zapisu danych służy klasa `pyspark.sql.DataFrameWriter`, która jest tworzona poprzez wywołanie metody `write()` klasy `DataFrame`. Najważniejsze metody to:\n",
    "\n",
    "`saveAsTable(name, format=None, mode=None, partitionBy=None, **options)`\n",
    "1. `name` - nazwa tabeli\n",
    "2. `mode` - tryb zapisu: \n",
    "\n",
    " *append*: dodaje nowe wiersze\n",
    " \n",
    " *overwrite*: nadpisuje dane\n",
    " \n",
    " *ignore*: jeśli dane istnieją w tabeli nie robi nic\n",
    " \n",
    " *error* (domyślnie): zwraca wyjątek, jeśli dane istnieją\n",
    " \n",
    "3. *partitionBy* - nazwy kolumn z partycjami\n",
    "4. `**options` - inne opcje\n",
    "\n",
    "`insertInto(tableName, overwrite=False)`\n",
    "1. `tableName` - nazwa tabeli\n",
    "2. `overwrite` - czy nadpisywać dane\n",
    "\n",
    "Przeanalizujmy następujące przykłady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1920cde-c214-4961-9e14-baa33f0cb464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usuwanie danych - potrzebne, gdy kilka razy wykonujemy polecenia w notebook'u\n",
    "dbutils.fs.rm(\"/user/hive/warehouse/uam_categories_sample/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003b4c7a-03f7-4adf-a2f0-889941513795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists default.uam_categories_sample\")\n",
    "\n",
    "# zapis danych do nowej tabeli\n",
    "uam_categories.sample(fraction=0.1).write.saveAsTable(\n",
    "    \"default.uam_categories_sample\", format=\"orc\"\n",
    ")\n",
    "# Dodanie nowych rekordów do tabeli\n",
    "uam_categories.sample(fraction=0.1).write.insertInto(\"default.uam_categories_sample\")\n",
    "\n",
    "# Zapis danych do tabeli partycjonowanej\n",
    "uam_categories.sample(fraction=0.1).write.saveAsTable(\n",
    "    \"default.uam_categories_sample\",\n",
    "    format=\"orc\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionedBy=[\"category_level1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b77843-308f-461e-90ce-4bf7b18cb440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dane możemy również zapisywać bezpośrednio do plików:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e969b6-fc51-4aa4-a42e-63d0bc6e5a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_offers.sample(False, 0.01).write.partitionBy(\"duration\").mode(\"overwrite\").parquet(\n",
    "    path=\"dbfs:/tmp/uam_offers_sample\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9793689b-b05f-4030-9e6f-641fef80f99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analogicznie zapis może być dokonany do formatów: *orc, csv, text, json* oraz do źródeł danych *JDBC* (metoda `jdbc`)\n",
    "\n",
    "W przypadku, gdy mamy zdefiniowaną tabelę w metastore, ale zdecydowaliśmy się pisać bezpośrednio do plików (jak w powyższym przykładzie), to dane mogę nie być widoczne. Poniższe polecenie sprawi, że metastore odświeży swoje metadane i uwzględni nowe pliki:\n",
    "```python\n",
    "spark.catalog.recoverPartitions(nazwa_tabeli)\n",
    "```\n",
    "Powyższe polecenie jest odpowiednikiem SQL'owego `MSCK REPAIR TABLE ...`\n",
    "<br><br><br>\n",
    "Czasami może się zdarzyć, że odczytujemy dane z pewnej tabeli, potem ją modyfikujemy (albo robi inny program), a potem znowu próbujemy ją odczytać. W przypadku, gdy spark zwróci wyjątek należy wykonać:\n",
    "```python\n",
    "spark.catalog.refreshTable(nazwa_tabeli)\n",
    "```\n",
    "Jest to odpowiednik SQL'owego `REFRESH TABLE ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f39a230-29f7-423c-a082-2f81ce2b0359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pandas & Koalas\n",
    "\n",
    "Biblioteka pandas jest jedną z najpolularniejszych bibliotek pythonowych do analizy danych (zob.: https://pandas.pydata.org/). Aby z niech korzystać wystarczy wywołać metodą `toPandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc9cdd3-72f1-492a-a649-f4d9cef04116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uam_categories.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b7b0cd-1e95-4a28-a9aa-e8058aac5cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Warto pamiętać że `toPandas` jest akcją. Oznacza to, że wszystkie dane są zbierane do pamięcie drivera (działa podobnie jak `collect`). Od najnowszych wersji Spark'a (3.2) możemy korzystać z tzw. Pandas API. Zapewnia ono podobne API jak `pandas`, ale przetwarzania mają charakter rozproszony. Oznacza to, że nie ogranicza nas pamięć driver'a :) Poniżej znajduje się przykład, w którym jest tworzony tzw. `pandas-on-Spark DataFrame`.\n",
    "\n",
    "Przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67d48e2-f582-45e8-82e1-86a7f278a8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_on_spark_df = uam_categories.pandas_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060bb717-efc7-4522-92c1-b3cbff2e6443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "W łatwy sposób możemy wrócić do Spark'owego DataFrame'a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374cfc41-5ae9-42b6-a349-343683125b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pandas_on_spark_df.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c327b6bb-99d3-40fd-ab4e-f8a485f5a05e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Poniżej przykład, który pokazuje możliwości Pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ad6429-83ab-4d25-add5-5b4b828df03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_on_spark_df[[\"category_id\",\"category_level1\"]].iloc[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199437d3-d064-412b-ac84-746673990386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dane są wyświetlane na ekran. Oznacza, że mamy do czynienia z akcją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702ce8b7-6266-432a-ab4d-02c040bc7616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Testowanie\n",
    "\n",
    "[Tutaj](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html) można znaleźć więcej przykładów osadzonych w kontekście popularnych frameworków do tetowania tj. `pytest` i `unittest`.\n",
    "\n",
    "Skupmy się na prostym przykładzie jak można sprawdzić, że nasz kod sparkowy przekształca dane zgodnie z oczekiwaniem.\n",
    "\n",
    "Przykład"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef147feb-91e7-4440-b84f-9ad8e88ef3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Oryginalne zapytanie stworzone w notebooku nie jest dobrym przykładem kodu, który łatwo testować:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f44ae6-938f-4e5f-a96b-fa941823a1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Przykład grupowania z części dot groupBy:\n",
    "# Min, Max cena oferty per kategoria\n",
    "uam_offers.join(\n",
    "    uam_categories, uam_categories.category_id == uam_offers.category_leaf\n",
    ")\n",
    ".groupBy(uam_categories.category_level1)\n",
    ".agg(\n",
    "    min(uam_offers.buynow_price.cast(\"double\")).alias(\"min_price\"),\n",
    "    max(uam_offers.buynow_price.cast(\"double\")).alias(\"max_price\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db67c4cb-c0c9-44e2-8514-f4acb110f888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zmieńmy je zgodnie z dobrymi praktykami tworząc fukcję. \n",
    "\n",
    "UWAGA: tego typu kod raczej nie tworzymy w notebookach, ale korzystając z IDE (PyCharm, VS Code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f450fdb4-2d3c-4a43-88b0-e45770b07064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def offer_stats_per_category(offers, categories):\n",
    "  return (\n",
    "      offers.join(\n",
    "      categories, categories.category_id == offers.category_leaf\n",
    "    )\n",
    "    .groupBy(categories.category_level1)\n",
    "    .agg(\n",
    "        min(offers.buynow_price.cast(\"double\")).alias(\"min_price\"),\n",
    "        max(offers.buynow_price.cast(\"double\")).alias(\"max_price\"),\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "537820bf-3bdb-4342-a325-3daf1f01cdff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sprawdźmy działanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d57aa64-6a76-424f-8031-ebe2c121303a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  offer_stats_per_category(uam_offers, uam_categories)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070aa787-ec6f-49cf-a583-5529d34e1cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Przejdźmy do testowania. W takim przypadku nie bazujemu na danych w tabelach. Musimy przygotować przypadki dane testowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f691b2e3-1075-45e8-81da-46dc06b2d5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "offer_test_df = spark.createDataFrame(\n",
    "  [\n",
    "    # Możemy ograniczyć się do kolumn występujących w DF, których używamy w przetwarzaniu\n",
    "    {\"category_leaf\": 10, \"buynow_price\": 10.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 20.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 30.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 40.00},\n",
    "    {\"category_leaf\": 10, \"buynow_price\": 100.00},\n",
    "  ]\n",
    ")\n",
    "\n",
    "categories_test_df = spark.createDataFrame(\n",
    "  [\n",
    "    # Możemy ograniczyć się do kolumn występujących w DF, których używamy w przetwarzaniu\n",
    "    {\"category_id\": 10, \"category_level1\": \"kat A\"},\n",
    "    {\"category_id\": 20, \"category_level1\": \"kat B\"},\n",
    "    {\"category_id\": 30, \"category_level1\": \"kat C\"},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398f1727-5165-41a6-b771-828fb354ca00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zobaczmy jak to działa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dec88c2-f144-4829-b3f5-158a5e5e9893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  offer_stats_per_category(offer_test_df, categories_test_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f2f4b9-0e19-4b89-bb76-1ca98cfa6b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "offer_stats_per_category(offer_test_df, categories_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aabdd8b-b11d-424e-9d56-2922f57f77e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nie chcemy jednak testować kodu oczami! Wykorzystajmy funkcję `assertDataFrameEqual`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b998553c-c700-4833-a3ed-764e7db68b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "expected_output_df = spark.createDataFrame(\n",
    "  [\n",
    "    {\"category_level1\": \"kat A\", \"max_price\": 100.0, \"min_price\": 10.0},\n",
    "    {\"category_level1\": \"kat B\", \"max_price\": 40.0, \"min_price\": 20.0},\n",
    "  ],\n",
    ")\n",
    "\n",
    "result_df = offer_stats_per_category(offer_test_df, categories_test_df)\n",
    "\n",
    "# kolejność kolumn ma znaczenie, przy porównywaniu DF\n",
    "columns_to_check = [\"category_level1\", \"max_price\", \"min_price\"]\n",
    "\n",
    "assertDataFrameEqual(\n",
    "  actual=result_df.select(*columns_to_check),\n",
    "  expected=expected_output_df.select(*columns_to_check)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe76620-8cd5-4737-bdb0-bb4f2c174360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ćwiczenie\n",
    "\n",
    "Zmodyfikuj `offer_stats_per_category`, aby dodatkowo wyliczana była średnia cena. \n",
    "Zmodyfikuj również testy, aby uwzględniały nowe wymaganie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60c49c5-f9ac-4dea-81a8-5226e31dcffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04bc6018-b87f-414b-ac7d-a4b5b56bfc52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2501447-33f0-436b-a611-628e4b91bbef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kod produkcyjny\n",
    "\n",
    "from pyspark.sql.functions import avg, max, min\n",
    "\n",
    "def offer_stats_per_category(offers, categories):\n",
    "  return (\n",
    "      offers.join(\n",
    "      categories, categories.category_id == offers.category_leaf\n",
    "    )\n",
    "    .groupBy(categories.category_level1)\n",
    "    .agg(\n",
    "        min(offers.buynow_price.cast(\"double\")).alias(\"min_price\"),\n",
    "        max(offers.buynow_price.cast(\"double\")).alias(\"max_price\"),\n",
    "        avg(offers.buynow_price.cast(\"double\")).alias(\"avg_price\"),   \n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264b6e97-54ff-4e27-87f0-9620207ed9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kod testowy\n",
    "\n",
    "# GIVEN:\n",
    "# bez zmian :)\n",
    "offer_test_df = spark.createDataFrame(\n",
    "  [\n",
    "    {\"category_leaf\": 10, \"buynow_price\": 10.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 20.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 30.00},\n",
    "    {\"category_leaf\": 20, \"buynow_price\": 40.00},\n",
    "    {\"category_leaf\": 10, \"buynow_price\": 100.00},\n",
    "  ]\n",
    ")\n",
    "\n",
    "categories_test_df = spark.createDataFrame(\n",
    "  [\n",
    "    {\"category_id\": 10, \"category_level1\": \"kat A\"},\n",
    "    {\"category_id\": 20, \"category_level1\": \"kat B\"},\n",
    "    {\"category_id\": 30, \"category_level1\": \"kat C\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# WHEN:\n",
    "result_df = offer_stats_per_category(offer_test_df, categories_test_df)\n",
    "\n",
    "# THEN:\n",
    "expected_output_df = spark.createDataFrame(\n",
    "  [\n",
    "    {\"category_level1\": \"kat A\", \"max_price\": 100.0, \"min_price\": 10.0, \"avg_price\": 55.0},\n",
    "    {\"category_level1\": \"kat B\", \"max_price\": 40.0, \"min_price\": 20.0, \"avg_price\": 30.0},\n",
    "  ],\n",
    ")\n",
    "\n",
    "columns_to_check = [\"category_level1\", \"max_price\", \"min_price\"]\n",
    "\n",
    "assertDataFrameEqual(\n",
    "  actual=result_df.select(*columns_to_check),\n",
    "  expected=expected_output_df.select(*columns_to_check)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345d5070-4408-404b-834c-f10d302902d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Porównanie różnych API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a892b9f1-9d56-4e48-a15e-a5f33fde10af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) RDD - imperatywne API\n",
    "uam_categories.rdd.filter(lambda r: r.category_level1 == \"Elektronika\").map(\n",
    "    lambda r: (r.category_level2, 1)\n",
    ").reduceByKey(lambda v1, v2: v1 + v2).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07879559-7e4c-4eea-9caa-199123ac6498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Spark SQL - deklaratywne API\n",
    "display(\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "          select category_level2,  count(1)\n",
    "          from uam_categories where category_level1 = 'Elektronika'\n",
    "          group by category_level2\n",
    "        \"\"\".strip()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063a7357-ec82-472d-adde-61cd754cc09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) DataFrame - deklaratywne API\n",
    "display(\n",
    "    uam_categories.where(uam_categories.category_level1 == \"Elektronika\")\n",
    "    .groupBy(\"category_level2\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f36ec2-f054-403b-af95-0bf2973cbd22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) Pandas API\n",
    "pandas_uam_categories = uam_categories.pandas_api()\n",
    "\n",
    "pandas_uam_categories[pandas_uam_categories.category_level1 == \"Elektronika\"].groupby(\n",
    "    [\"category_level2\"]\n",
    ").size()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "PySpark - workshop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
