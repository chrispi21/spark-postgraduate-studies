{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d49b1c-7a22-4b4d-9494-f53d9c0424ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- \n",
    "This work includes material from Apache Spark documentation licensed under Apache License 2.0. \n",
    "Original content available at: https://spark.apache.org/documentation.html \n",
    "\n",
    "This document contains modified excerpts from Apache Spark documentation, originally licensed under Apache License 2.0. Modifications made by Krzysztof Płatek.\n",
    "\n",
    "-->\n",
    "\n",
    "# Co to jest Spark?\n",
    "\n",
    "* framework do rozproszonego przetwarzania dużych zbiorów danych\n",
    "* dane są przetwarzane w pamięci operacyjnej\n",
    "* wbudowany optymalizator\n",
    "* obsługuje różne źródła danych\n",
    "  * JDBC\n",
    "  * pliki: JSON, Avro, Parquet, Delta, Iceberg, ORC, CSV pobierane z różnych zasobów dyskowych:\n",
    "    * HDFS, S3, ADLS, GCS\n",
    "  * Cassandra\n",
    "  * MongoDB\n",
    "  * ...\n",
    "* napisany w języku Scala\n",
    "  * co implikuje uruchomienie na maszynie wirtualnej Java'y (JVM)\n",
    "* wsparcie dla Python (PySpark), SQL, Scala, Java, R\n",
    "* przetwarzania wsadowe i strumieniowe\n",
    "* architektura master-worker\n",
    "\n",
    "Darmowa książka:\n",
    "1. https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf\n",
    "\n",
    "Docs:\n",
    "1. https://spark.apache.org/docs/latest/api/python/index.html\n",
    "2. https://spark.apache.org/docs/latest/index.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe64fbb-7883-4b81-a445-497a7f1fdb8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Komponenty aplikacji Spark'owej\n",
    "\n",
    "1. Aplikacja (*application*) - program napisany za pomocą Spark'a. Składa się z dwóch rodzajów komponentów:\n",
    "  * `driver` - \"zarządza\" aplikacją. Jest tylko jeden.\n",
    "  * `executor` - odpowiedzialny za wykonanie obliczeń. Może być ich wiele.\n",
    "2. `SparkSession` (sesja Spark) - obiekt, za pomocą którego korzystamy z możliwości Spark'a\n",
    "3. `Job` - wykonywane lub wykonane przetwarzanie zlecone Spark'owi\n",
    "4. `Stage` - `Job` wykonywane jest w etapach\n",
    "5. `Task` - obliczenia w ramach `Stage` wykonywane są w `Task`'ach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75280dce-d086-49c8-bf62-2766e67d4c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Uproszczona architektura aplikacji Spark\n",
    "\n",
    "![cluster-overview](https://raw.githubusercontent.com/chrispi21/spark-postgraduate-studies/main/resources/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa0805a1-2b70-452c-92d0-bce9045972d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managerowie klastra (`cluster managers`)\n",
    "\n",
    "Manager klastra dpowiada za przydział zasobów obliczeniowych:\n",
    "1. `Standalone`\n",
    "2. `Mesos` - przestarzały\n",
    "3. `YARN`\n",
    "4. `Kubernetes`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87e9632-d0f1-42b7-8233-849a899d138b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44dfc8ac-93af-4f83-92ad-edb9f61ac28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obiekt `SparkSession` jest dostępny w notebook'u pod nazwą `spark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573e3722-add3-4cfe-8214-4f4c893e0656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6305c553-39c5-4673-aa40-ee90d42e595d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job vs Stage vs Task\n",
    "\n",
    "Web UI demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1efa6dd2-bdd4-448f-b367-b41b693948c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Job #1\n",
    "spark.sql(\"select 1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f48d8d5-6f6e-4627-8baf-09f4463b0bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Job #2\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "df1 = spark.range(1000000).selectExpr(\"(rand() * 100) % 10 as mod10\").groupby(\"mod10\").count().orderBy(\"mod10\")\n",
    "df2 = spark.range(20000000).selectExpr(\"id % 20 as mod20\").groupby(\"mod20\").count().orderBy(\"mod20\")\n",
    "display(\n",
    "  df1.join(df2, on=(df1[\"mod10\"] == df2[\"mod20\"]))\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13614ee5-3fc0-4859-b3d8-129f81781adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Jak Spark przechowuje dane w pamięci?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36a02a22-d6b2-452a-8d48-658a9702774a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podział danych na partycje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f76b0e-2be9-4a10-9dc5-9602c6172420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# w tym przykładzie wymuszamy ręcznie podział danych na 32 partycje - standardowo Spark robi to automatycznie\n",
    "df = spark.range(1000, numPartitions=32)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356155d5-962d-40e8-81a6-ee363638706e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Zobaczmy jak wpływa to na liczbę tasków?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a118a4-e9aa-41f9-8e0c-4a96282cb6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6789b664-597e-49d7-946c-7cb37f28e73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Leniwe wykonanie kodu (`lazy evaluation`)\n",
    "\n",
    "Co to jest `DataFrame` w Spark'u?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550156d3-bc27-4e5f-8120-c2dca712abde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"select 1 as id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740e1b20-b76f-4512-acf9-6dbf2ee628a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"select 2 as id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371cfa7c-25c9-4109-98b4-db53e942d6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e464e9-3a21-4a75-9816-f94f9b447683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Co się dzieje?\n",
    "\n",
    "Akcja (`action`) vs. Transformacja (`transformation`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ec686b-f255-4f44-8f83-83dd69359838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce4963e1-662a-4bec-9aaf-a84bc04f0cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD\n",
    "\n",
    "RDD jest abstrakcją, na której oparte są `DataFrame`'y. Skupmy się wyłącznie na ważnych właściwościach:\n",
    "\n",
    "* Resilient - odporność na awarię - możliwość ponownego przeliczenia części transformacji bez konieczności przeliczania całości\n",
    "* Distributed - rozproszony - dane są przechowywane na wielu węzłach\n",
    "* Dataset - zbiór danych\n",
    "\n",
    "RDD też są wykonywane leniwie, ale nie mają właściwości `DataFrame`, jeśli chodzi o optymalizację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3e2545-3023-48ee-8e46-e8ef3d5b27b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wąskie i szerokie transformacje (`narrow and wide transformations`)\n",
    "\n",
    "1. Wąskie - jedna partycja wyjściowa powstaje na bazie jednej partycji wejściowej. Przykłady: filtrowanie, projekcja\n",
    "2. Szerokie - jedna partycja wyjściowa powstaje na bazie wielu partycji wejściowych. Przykłady: grupowanie, sortowanie, łączenie\n",
    "\n",
    "Transformacje wąskie nie wymagają przesyłu danych po sieci.\n",
    "\n",
    "Transformacje szerokie wymagają wymiany danych między executorami (tzw. `shuffle`).\n",
    "\n",
    "Porównajmy czas wykonania i metryki związane z poniższymi przykładami:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0173d10e-9381-4f0b-81f0-795794384534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.range(10_000_000)\n",
    "df2 = df1.selectExpr(\"1000*rand() % 100 as mod100\")\n",
    "df2.take(1000)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd927210-d3eb-49f1-9e50-a1ce6c47bef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.range(10_000_000)\n",
    "df2 = df1.selectExpr(\"1000*rand() % 100 as mod100\").groupBy(\"mod100\").count().orderBy(\"mod100\")\n",
    "df2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199b5ac9-c14e-41e2-b863-27307829de2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serializacja i deserializacja (`serialization`, `deserialization`)\n",
    "\n",
    "W skrócie: SerDe\n",
    "\n",
    "Są to operacje, które są niezbędne do sieciowego przesłania danych. Mają ogromne znaczenie dla wydajności przy wymianie danych między executorami (`shuffle`).\n",
    "\n",
    "Spark korzysta z własnej reprezentacji danych w pamięci, która jest zoptymalizowana pod kątem szybkiej serializacji i deserializacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35562e9f-5bda-411d-89c3-80b65f0f0551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serializacja\n",
    "\n",
    "Aby przesłać dane przez sieć obiekt musi zostać zamieniony na postać binarną. Mówimy wtedy, że obiekt został zserializowany.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "obiekt -> postać binarna\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f46b7dc-6e9e-40b1-a4d0-ad6f1803c0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deserializacja\n",
    "Odbiorca wiadomości odczytuje dane przesłane siecią w postaci binarnej i zamienia na obiekt:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "postać binarna -> obiekt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85283ef7-59a2-4222-bab6-7716861aad24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Encoders\n",
    "\n",
    "W poprzedniej sekcji padło stwierdzenie:\n",
    "\n",
    "\"Spark korzysta z własnej reprezentacji danych w pamięci, która jest zoptymalizowana pod kątem szybkiej serializacji i deserializacji.\"\n",
    "\n",
    "Jak to się ma to reprezentacji JVM?\n",
    "\n",
    "Tutaj również ma miejsce proces serializacji i deserializacji, który obsługuje wewnętrzny mechanizm enkoderów (`encoders`, `dataset encoders`). Najczęściej możemy uniknąć SerDe, korzystając z funkcji wbudowanych w Spark'a albo wyrażeń SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29338726-5128-4cee-9a5a-287a7b0c1a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funkcja wbudowana - unikamy SerDe\n",
    "from pyspark.sql.functions import pmod\n",
    "\n",
    "spark.range(100).select(pmod(\"id\", 10)).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b82d12-65dd-4470-8bf3-61dd0202fed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL - unikamy Serde\n",
    "EXPLAIN EXTENDED\n",
    "  SELECT mod(id, 10)\n",
    "  FROM RANGE(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfcbb5a-6d5f-46b4-a4d8-53c0e8a80ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Pojawia się DeserializeToObject - SerDe: Java <-> Spark\n",
    "spark.range(1000).map(id => id % 10).explain(extended=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69a1875-ff24-4ee3-b6fd-40e8fa47dcf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Tak nie robimy - są lepsze sposoby na własne funkcje w PySpark!\n",
    "\n",
    "# Pojawia się BatchEvalPython\n",
    "# Po sprawdzeniu:\n",
    "# https://github.com/apache/spark/blob/v3.5.0/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala#L103\n",
    "# Oznacza to SerDe: interpreter Python <-> JVM oraz JVM <-> Spark\n",
    "from pyspark.sql.functions import udf\n",
    "mod10 = udf(lambda x: x % 10)\n",
    "spark.range(1000).select(mod10(\"id\")).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1203a626-874e-46cd-8638-c2048c1d53ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Catalyst\n",
    "\n",
    "Optymalizator kosztowy wbudowany w Spark'a (CBO - `Cost Based Optimizer`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b4774b-1229-41f1-9e74-5b97289fe70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fazy optymalizacji zapytania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "812ae7f2-b7af-4ad7-bc21-3f5aa78e6ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analiza\n",
    "\n",
    "Sprawdzana jest poprawność składniowa oraz poprawność nazw tabel i kolumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b7c030-e886-47c9-b73e-02b1abd0dcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Błąd\n",
    "spark.sql(\"select \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7349b0b7-6879-42c2-965e-d27d20c74faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Błąd\n",
    "spark.sql(\"select 1 as col1\").where(\"colXYZ = 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30864e4e-d2dc-44e6-aa40-568eb0a99887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logiczna optymalizacja\n",
    "\n",
    "W tym momemcie generowane są alternatywne plany zapytań uwzględniając szereg reguł optymalizacyjnych (optymalizator regułowy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56aecf7-97de-4c53-9f50-a92b3400ae3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# interesuje nas sekcja: == Optimized Logical Plan ==\n",
    "\n",
    "spark.sql(\"select 1 as col1\").where(\"col1 = 5\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19260166-1bb7-49b4-9f7c-0daaa337e163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Plan fizyczny\n",
    "\n",
    "Generowane są plany fizyczne na bazie planu logicznego. Każdy plan ma obliczany koszt z nim związany. Wybierany jest ten z najniższym kosztem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f07e69-05de-4179-ab04-376a1c770c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# interesuje nas sekcja: == Physical Plan ==\n",
    "\n",
    "spark.sql(\"select 1 as col1\").where(\"col1 = 5\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6e500b-fb67-4ba1-b714-b3c1108f03d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generowanie kodu\n",
    "\n",
    "Na bazie optymalnego planu fizycznego generowany jest kod bajtowy (`bytecode`). Dlaczego? Plan fizyczny jest skomplikowany tj. jego wykonanie wymagałoby tworzenia wielu nowych obiektów w pamięci. Ten etap ma na celu wygenerowanie kodu, który jest optymalny pod kątem wykonania na maszynie wirtualnej Java'y (JVM). Wiele tego typu optymalizacji (i nie tylko) obejmuje [Project Tungsten](https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981b4075-d4f4-4c9c-9380-7ae13def7df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select 1 as col1\").where(\"col1 = 5\").explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2903efc8-ca6a-4c0d-8d3a-4eba51afcb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.range(1000).explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca7fc73-5027-49ff-84c7-17e101866d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Adaptive Query Execution (AQE)\n",
    "\n",
    "Domyślnie Spark optymalizuje plan wykonania bazując na statystykach już wykonanych etapów. \n",
    "\n",
    "Więcej szczegółów:\n",
    "\n",
    "https://www.databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355ae0e3-7254-434c-9070-eae2653437e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: SQL/Dataframe\n",
    "\n",
    "df1 = spark.range(1000000).selectExpr(\"(rand() * 100) % 10 as mod10\").groupby(\"mod10\").count().orderBy(\"mod10\")\n",
    "df2 = spark.range(20000000).selectExpr(\"id % 20 as mod20\").groupby(\"mod20\").count().orderBy(\"mod20\")\n",
    "display(\n",
    "  df1.join(df2, on=(df1[\"mod10\"] == df2[\"mod20\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeae0bf3-79a9-42bc-9cf2-7eeb0718c220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1585298298230635,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Spark intro",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
